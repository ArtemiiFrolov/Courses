\documentclass[11pt]{article}

% ------------------------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% ------------------------------------------------------------------------------

\usepackage[margin=.8in,top=1.1in,bottom=1.1in]{geometry} % page layout
\usepackage{amsmath,amsthm,amssymb,amsfonts} % math things
\usepackage{graphicx} % include graphics
\usepackage{fancyhdr} % header customization
\usepackage{titlesec} % help with section naming

% naming sections
\titleformat{\section}{\bf}{Problem \thesection}{0.5em}{}
\newcommand{\exercise}{\section{}}

% headers
\pagestyle{fancy} 
\fancyhf{} % clear all
\fancyhead[L]{\sffamily\small Machine Learning 1 --- Homework}
\fancyhead[R]{\sffamily\small Page \thepage}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.2pt}
\markright{\hrulefill\quad}

\newcommand{\hwhead}[4]{
\begin{center}
\sffamily\large\bfseries Machine Learning Worksheet #1
\vspace{2mm} 
\normalfont

#2 -- #3 -- \texttt{#4}
\end{center}
\vspace{6mm} \hrule \vspace{4mm}
}

% ------------------------------------------------------------------------------
% Start here -- Fill in your name, imat and email
% ------------------------------------------------------------------------------

\newcommand{\name}{Artemii Frolov} %
\newcommand{\imat}{03681119} %
\newcommand{\email}{ga83cag@mytum.de} %

\begin{document}

% ------------------------------------------------------------------------------
% Change xx (and only xx) to the current sheet number
% ------------------------------------------------------------------------------
\hwhead{xx}{\name}{\imat}{\email}

% ------------------------------------------------------------------------------
% Fill in your solutions
% ------------------------------------------------------------------------------

\exercise % each new exercise begins with this command
As we know, hard margin SVM will find a correct separation for every linearly separable dataset $D$. The main difference from soft-margin SVM is constant size of margin - it equals to maximum distance between two closest points with different labels. So, when we will try to do soft-margin SVM for linearly separable dataset, we will still penalize "wrong" or "in margin" labels, but with different $C$ we can deal with some situation, where we have no need of linear separability, even if the set is like this. \newline
Let's assume we have a dataset of two classes - and each class is really far away of each other. But one point of class A is noisy, and (it happens) became really close to class B, but set can still be linearly separated. In this situation we can use soft-margin SVM with low $C$ component to make a hyperplane, that penalize a little bit wrong label of that noisy point, but with respect to high distance between two classes (which means wide margin) still separate it not perfectly (in terms of mathematic way, not logical). \newline
In the graphic below it should be read as "soft margin center hyperplane" and "hard margin center hyperplane"
\includegraphics{graphic}
\exercise
Via the formula and slack variables, if $C<0$ we will penalize functions for right decisions. It means, that central hyperplane will be: a) wrong side "rotated"  and b) as far as it can be possible from data. \newline
If $C=0$ we have no constraints of $\xi$, so this means that slack variable can be any value, so with given constraints there can be any consequences of final hyperplane.

\exercise
$$(x^Ty+c)^d = \prod_{i=1}^{d>0}x^Ty+c$$
$x^Ty$ is a kernel and $c>0$ is a kernel, that mean $x^Ty+c$ is also a kernel. Product of kernels is also a kernel.

\exercise
With given infinite transformation we cannot apply this feature to data, because we need store somewhere all the points of this basis function. If there infinite points, we need infinite storage and time to compute all possible transformations, which is impossible (at present time).

\exercise
$$\sum_{i=0}^{\infty}\frac{e^{-x^2/2\sigma^2}(\frac{x}{\sigma})^i}{\sqrt{i!}}*\frac{e^{-y^2/2\sigma^2}(\frac{y}{\sigma})^i}{\sqrt{i!}}$$
$$=e^{-\frac{x^2+y^2}{2\sigma^2}}\sum_{i=0}^{\infty}\frac{(\frac{xy}{\sigma^2})^i}{i!}$$
$$=e^{-\frac{x^2+y^2}{2\sigma^2}}e^{\frac{xy}{\sigma^2}}=e^{-\frac{(x-y)^2}{2\sigma^2}}$$
If we will choose wrong $\sigma$ we should be concern about overfitting. While it radial kernel, with small $\sigma$ we can receive small circles around all points of class A, while all other space will  be class B, which of course wrong at the most situations.
\exercise
Yes. As it's been written at problem 5, with small $\sigma$ we can receive situation, where all points of one of the classes will be in a really small circles of minimum size, which make it linearly separable with finite points.
\exercise
We know, that for kNN algorithm, with y={-1;1} it should be look like:
$$t=\sum^k_iy_id_i$$
where $d_i$ is parameter, that is inversely proportional  to a distance between $x$ and $x_i$. Previously, we used $d_i=\frac{1}{||x-x_i||_2}$, but we can introduce another function of $d$, e.g. $d_i=e^{-||x-x_i||^2}$.\newline
As long as we know, that Gaussian kernel is $K(x,y)=e^{-\frac{||x-y||^2}{2\sigma^2}}$, we can assume $\sigma=\frac{1}{\sqrt{2}}$, so $K(x,y)=e^{-||x-y||^2}$. It's look the same as $d$, so we can rewrite kNN algorithm as $t=\sum^k_iy_iK(x,x^{(i)})$, where $K(x,x^{(i)}=e^{-||x-x_i||^2}=\phi(x)^t\phi(x^{(i)})$. In this case $\phi(x)=e^{-x^2}[\sqrt{\frac{2^i}{i!}}x^i]_{i=0..\infty}$
\end{document}
