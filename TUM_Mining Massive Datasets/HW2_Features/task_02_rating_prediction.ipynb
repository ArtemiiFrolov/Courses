{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project task 02: Restaurant recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "import time\n",
    "import random\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this task is to recommend restaurants to users based on the rating data in the Yelp dataset. For this, we try to predict the rating a user will give to a restaurant they have not been to yet based on a latent factor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download `ratings.npy` from Piazza ([download link](https://syncandshare.lrz.de/dl/fiKMoxRNusLoFpFHkXXEgvdZ/ratings.npy))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = np.load(\"ratings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[101968,   1880,      1],\n",
       "       [101968,    284,      5],\n",
       "       [101968,   1378,      2],\n",
       "       ...,\n",
       "       [ 72452,   2100,      4],\n",
       "       [ 72452,   2050,      5],\n",
       "       [ 74861,   3979,      5]], dtype=uint32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have triplets of (user, restaurant, rating).\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform the data into a matrix of dimension [N, D], where N is the number of users and D is the number of restaurants in the dataset.  \n",
    "We **strongly recommend** to load the data as a sparse matrix to avoid out-of-memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the matrix into the variable M\n",
    "M = sp.csr_matrix((ratings[:,2], (ratings[:,0], ratings[:,1])),dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<337867x5899 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 929606 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preprocessing step, we recursively remove all users and restaurants with 10 or less ratings. \n",
    "\n",
    "Then, we randomly select 200 data points for the validation and test sets, respectively.\n",
    "\n",
    "After this, we subtract the mean rating for each users to account for this global effect.   \n",
    "**Hint**: Some entries might become zero in this process -- but these entries are different than the 'unknown' zeros in the matrix. Store the indices of which we have data available in a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cold_start_preprocessing(matrix, min_entries):\n",
    "    \"\"\"\n",
    "    Recursively removes rows and columns from the input matrix which have less than min_entries nonzero entries.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix      : sp.spmatrix, shape [N, D]\n",
    "                  The input matrix to be preprocessed.\n",
    "    min_entries : int\n",
    "                  Minimum number of nonzero elements per row and column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix      : sp.spmatrix, shape [N', D']\n",
    "                  The pre-processed matrix, where N' <= N and D' <= D\n",
    "        \n",
    "    \"\"\"\n",
    "    print(\"Shape before: {}\".format(matrix.shape))\n",
    "    \n",
    "    # Calculating binary values for each row an column\n",
    "    rows = matrix.getnnz(1)>min_entries\n",
    "    cols = matrix.getnnz(0)>min_entries\n",
    "    \n",
    "    # If any value of binary vector has False in it, delete rows/cols that have False value\n",
    "    while (not rows.all()) or (not cols.all()):\n",
    "        matrix = matrix[rows][:,cols]\n",
    "        rows = matrix.getnnz(1)>min_entries\n",
    "        cols = matrix.getnnz(0)>min_entries\n",
    "\n",
    "        \n",
    "    print(\"Shape after: {}\".format(matrix.shape))\n",
    "    nnz = matrix>0\n",
    "    assert (nnz.sum(0).A1 > min_entries).all()\n",
    "    assert (nnz.sum(1).A1 > min_entries).all()\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_user_mean(matrix):\n",
    "    \"\"\"\n",
    "    Subtract the mean rating per user from the non-zero elements in the input matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             Input sparse matrix.\n",
    "    Returns\n",
    "    -------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             The modified input matrix.\n",
    "    \n",
    "    user_means : np.array, shape [N, 1]\n",
    "                 The mean rating per user that can be used to recover the absolute ratings from the mean-shifted ones.\n",
    "\n",
    "    \"\"\"\n",
    "    rows, cols = np.nonzero(matrix)\n",
    "    user_means = np.zeros((matrix.shape[0], 1))\n",
    "    for i in range(matrix.shape[0]):\n",
    "        if matrix[i].count_nonzero() > 0:\n",
    "            user_means[i] = matrix[i].sum()/matrix[i].count_nonzero()\n",
    "        else:\n",
    "            user_means[i] = 0\n",
    "    for i, row in enumerate(rows):\n",
    "        matrix[row, cols[i]] -= user_means[row]\n",
    "\n",
    "    assert np.all(np.isclose(matrix.mean(1), 0))\n",
    "    return matrix, user_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(matrix, n_validation, n_test):\n",
    "    \"\"\"\n",
    "    Extract validation and test entries from the input matrix. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix          : sp.spmatrix, shape [N, D]\n",
    "                      The input data matrix.\n",
    "    n_validation    : int\n",
    "                      The number of validation entries to extract.\n",
    "    n_test          : int\n",
    "                      The number of test entries to extract.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix_split    : sp.spmatrix, shape [N, D]\n",
    "                      A copy of the input matrix in which the validation and test entries have been set to zero.\n",
    "    \n",
    "    val_idx         : tuple, shape [2, n_validation]\n",
    "                      The indices of the validation entries.\n",
    "    \n",
    "    test_idx        : tuple, shape [2, n_test]\n",
    "                      The indices of the test entries.\n",
    "    \n",
    "    val_values      : np.array, shape [n_validation, ]\n",
    "                      The values of the input matrix at the validation indices.\n",
    "                      \n",
    "    test_values     : np.array, shape [n_test, ]\n",
    "                      The values of the input matrix at the test indices.\n",
    "\n",
    "    \"\"\"\n",
    "    # Generating indices of nonzero entries\n",
    "    rows, cols = np.nonzero(matrix)\n",
    "    # Generatin random rows\n",
    "    rand_items = np.random.choice(rows.shape[0], n_validation+n_test, replace=False)\n",
    "    val_idx = (rows[rand_items[:n_validation]], cols[rand_items[:n_validation]])    \n",
    "    test_idx = (rows[rand_items[n_validation:]], cols[rand_items[n_validation:]]) \n",
    "    val_values = matrix[val_idx]\n",
    "    test_values = matrix[test_idx]\n",
    "    matrix_split = matrix.copy()\n",
    "    matrix_split[val_idx] = 0\n",
    "    matrix_split[test_idx] = 0\n",
    "    matrix_split.eliminate_zeros()\n",
    "    return matrix_split, val_idx, test_idx, val_values, test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before: (337867, 5899)\n",
      "Shape after: (11275, 3531)\n"
     ]
    }
   ],
   "source": [
    "M = cold_start_preprocessing(M, 10)\n",
    "#Shape before: (337867, 5899)\n",
    "#Shape after: (11275, 3531)\n",
    "L = M.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_validation = 200\n",
    "n_test = 200\n",
    "# Split data\n",
    "M_train, val_idx, test_idx, val_values, test_values = split_data(M, n_validation, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store away the nonzero indices of M before subtracting the row means.\n",
    "nonzero_indices = np.nonzero(M_train)\n",
    "# Remove user means.\n",
    "M_shifted, user_means = shift_user_mean(M_train)\n",
    "# Apply the same shift to the validation and test data.\n",
    "val_values_shifted = val_values - user_means[val_idx[0]].T \n",
    "test_values_shifted = test_values - user_means[test_idx[0]].T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_n_shifted, _ = shift_user_mean(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Alternating optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step, we will approach the problem via alternating optimization, as learned in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Q_P(matrix, k, init='random'):\n",
    "    \"\"\"\n",
    "    Initialize the matrices Q and P for a latent factor model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             The matrix to be factorized.\n",
    "    k      : int\n",
    "             The number of latent dimensions.\n",
    "    init   : str in ['svd', 'random'], default: 'random'\n",
    "             The initialization strategy. 'svd' means that we use SVD to initialize P and Q, \n",
    "             'random' means we initialize the entries in P and Q randomly in the interval [0, 1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : np.array, shape [N, k]\n",
    "        The initialized matrix Q of a latent factor model.\n",
    "\n",
    "    P : np.array, shape [k, D]\n",
    "        The initialized matrix P of a latent factor model.\n",
    "    \"\"\"\n",
    "\n",
    "    N, D = matrix.shape\n",
    "    if init == 'svd':  \n",
    "        Q, _, P = svds(matrix, k=k)\n",
    "    elif init == 'random':\n",
    "        Q = np.random.random((N, k))\n",
    "        P = np.random.random((k, D))\n",
    "    else:\n",
    "        raise ValueError\n",
    "        \n",
    "    assert Q.shape == (matrix.shape[0], k)\n",
    "    assert P.shape == (k, matrix.shape[1])\n",
    "    return Q, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_factor_alternating_optimization(M, non_zero_idx, k, val_idx, val_values,\n",
    "                                           reg_lambda, max_steps=100, init='random',\n",
    "                                           log_every=1, patience=10, eval_every=1):\n",
    "    \"\"\"\n",
    "    Perform matrix factorization using alternating optimization. Training is done via patience,\n",
    "    i.e. we stop training after we observe no improvement on the validation loss for a certain\n",
    "    amount of training steps. We then return the best values for Q and P oberved during training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    M                 : sp.spmatrix, shape [N, D]\n",
    "                        The input matrix to be factorized.\n",
    "                      \n",
    "    non_zero_idx      : np.array, shape [nnz, 2]\n",
    "                        The indices of the non-zero entries of the un-shifted matrix to be factorized. \n",
    "                        nnz refers to the number of non-zero entries. Note that this may be different\n",
    "                        from the number of non-zero entries in the input matrix M, e.g. in the case\n",
    "                        that all ratings by a user have the same value.\n",
    "    \n",
    "    k                 : int\n",
    "                        The latent factor dimension.\n",
    "    \n",
    "    val_idx           : tuple, shape [2, n_validation]\n",
    "                        Tuple of the validation set indices.\n",
    "                        n_validation refers to the size of the validation set.\n",
    "                      \n",
    "    val_values        : np.array, shape [n_validation, ]\n",
    "                        The values in the validation set.\n",
    "                      \n",
    "    reg_lambda        : float\n",
    "                        The regularization strength.\n",
    "                      \n",
    "    max_steps         : int, optional, default: 100\n",
    "                        Maximum number of training steps. Note that we will stop early if we observe\n",
    "                        no improvement on the validation error for a specified number of steps\n",
    "                        (see \"patience\" for details).\n",
    "                      \n",
    "    init              : str in ['random', 'svd'], default 'random'\n",
    "                        The initialization strategy for P and Q. See function initialize_Q_P for details.\n",
    "    \n",
    "    log_every         : int, optional, default: 1\n",
    "                        Log the training status every X iterations.\n",
    "                    \n",
    "    patience          : int, optional, default: 10\n",
    "                        Stop training after we observe no improvement of the validation loss for X evaluation\n",
    "                        iterations (see eval_every for details). After we stop training, we restore the best \n",
    "                        observed values for Q and P (based on the validation loss) and return them.\n",
    "                      \n",
    "    eval_every        : int, optional, default: 1\n",
    "                        Evaluate the training and validation loss every X steps. If we observe no improvement\n",
    "                        of the validation error, we decrease our patience by 1, else we reset it to *patience*.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_Q            : np.array, shape [N, k]\n",
    "                        Best value for Q (based on validation loss) observed during training\n",
    "                      \n",
    "    best_P            : np.array, shape [k, D]\n",
    "                        Best value for P (based on validation loss) observed during training\n",
    "                      \n",
    "    validation_losses : list of floats\n",
    "                        Validation loss for every evaluation iteration, can be used for plotting the validation\n",
    "                        loss over time.\n",
    "                        \n",
    "    train_losses      : list of floats\n",
    "                        Training loss for every evaluation iteration, can be used for plotting the training\n",
    "                        loss over time.                     \n",
    "    \n",
    "    converged_after   : int\n",
    "                        it - patience*eval_every, where it is the iteration in which patience hits 0,\n",
    "                        or -1 if we hit max_steps before converging. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Q, P = initialize_Q_P(M, k, init)\n",
    "    \n",
    "    N = M.shape[0]\n",
    "    D = P.shape[1]\n",
    "\n",
    "    item2nzusers = {}\n",
    "    user2nzitems = {}\n",
    "    converged_after = 0\n",
    "    for (u, i) in np.array(non_zero_idx).T:\n",
    "        if i not in item2nzusers:\n",
    "            item2nzusers[i] = []\n",
    "        item2nzusers[i].append(u)\n",
    "      \n",
    "        if u not in user2nzitems:\n",
    "            user2nzitems[u] = []\n",
    "        user2nzitems[u].append(i)\n",
    "          \n",
    "    best_Q = Q\n",
    "    best_P = P\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    lr = Ridge(alpha = reg_lambda)\n",
    "    curr_patience = patience\n",
    "    zero_time = time.time()\n",
    "    start_time = time.time()\n",
    "    best_val_loss = 0\n",
    "    for iteration in range(max_steps):        \n",
    "        \n",
    "        if iteration % eval_every == 0:\n",
    "            QP = Q.dot(P)\n",
    "            train_losses.append(np.sum(np.square(M[non_zero_idx] - QP[non_zero_idx])))\n",
    "            validation_losses.append(np.sum(np.square(val_values - QP[val_idx])))\n",
    "            if iteration == 0 or best_val_loss > validation_losses[-1]:\n",
    "                best_Q = Q\n",
    "                best_P = P\n",
    "                best_val_loss = validation_losses[-1]\n",
    "                curr_patience = patience\n",
    "                converged_after = iteration \n",
    "            else:\n",
    "                curr_patience -= 1\n",
    "      \n",
    "        if iteration % log_every == 0:\n",
    "            print(\"Iteration {:}, training loss: {:.2f}, validation loss = {:.2f}, time = {:.2f}\".\n",
    "                  format(iteration, train_losses[-1], validation_losses[-1], time.time()-start_time))\n",
    "            start_time = time.time()\n",
    "            \n",
    "        for d in range(D):\n",
    "            nzusers = item2nzusers[d]\n",
    "            P[:, d] = lr.fit(X=Q[nzusers,:], y=M[nzusers, d].toarray()).coef_\n",
    "\n",
    "        for u in range(N):\n",
    "            nzitems = user2nzitems[u]\n",
    "            Q[u, :] = lr.fit(X=P[:,nzitems].T,\n",
    "                             y=M[u, nzitems].toarray().T).coef_\n",
    "            \n",
    "        if curr_patience == 0 or max_steps==iteration:\n",
    "            break\n",
    "            \n",
    "    print(\"Converged after {:} iterations, on average {:.2f} per iteration\".\n",
    "          format(converged_after, (time.time() - zero_time)/iteration))\n",
    "    return best_Q, best_P, validation_losses, train_losses, converged_after\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the latent factor model with alternating optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Learn the optimal $P$ and $Q$ using alternating optimization. That is, during each iteration you first update $Q$ while having $P$ fixed and then vice versa. Run the alternating optimization algorithm with $k=100$ and $\\lambda=1$. Plot the training and validation losses over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss: 179994070.47, validation loss = 124779.84, time = 0.51\n",
      "Iteration 1, training loss: 31409.57, validation loss = 291.93, time = 23.23\n",
      "Iteration 2, training loss: 11182.24, validation loss = 285.26, time = 20.60\n",
      "Iteration 3, training loss: 8219.73, validation loss = 266.29, time = 18.69\n",
      "Iteration 4, training loss: 7204.58, validation loss = 253.78, time = 18.18\n",
      "Iteration 5, training loss: 6715.96, validation loss = 247.71, time = 18.57\n",
      "Iteration 6, training loss: 6434.72, validation loss = 245.86, time = 19.08\n",
      "Iteration 7, training loss: 6253.53, validation loss = 246.46, time = 18.77\n",
      "Iteration 8, training loss: 6126.62, validation loss = 247.95, time = 18.56\n",
      "Iteration 9, training loss: 6032.11, validation loss = 249.36, time = 18.31\n",
      "Iteration 10, training loss: 5958.34, validation loss = 250.38, time = 18.67\n",
      "Iteration 11, training loss: 5898.63, validation loss = 251.02, time = 17.51\n",
      "Iteration 12, training loss: 5848.99, validation loss = 251.33, time = 17.71\n",
      "Iteration 13, training loss: 5806.94, validation loss = 251.37, time = 17.64\n",
      "Iteration 14, training loss: 5770.78, validation loss = 251.21, time = 18.36\n",
      "Iteration 15, training loss: 5739.40, validation loss = 250.97, time = 19.18\n",
      "Iteration 16, training loss: 5711.97, validation loss = 250.77, time = 18.49\n",
      "Converged after 6 iterations, on average 20.06 per iteration\n"
     ]
    }
   ],
   "source": [
    "Q_a, P_a, val_l_a, tr_l_a, conv_a = latent_factor_alternating_optimization(M_shifted, nonzero_indices, \n",
    "                                                                           k=100, val_idx=val_idx,\n",
    "                                                                           val_values=val_values_shifted, \n",
    "                                                                           reg_lambda=1, init='random',\n",
    "                                                                           max_steps=100, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the validation and training losses over (training) time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24e15a90438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(val_l_a[1:])\n",
    "plt.title('Alternating optimization validation loss, k=100')\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('Validation loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(tr_l_a[1:])\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('Training loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) (**Optional**): Try some different latent dimensions $k$ in the range [5, 100]. What do you observe (convergence time, final training/validation losses)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss: 1084730.81, validation loss = 695.84, time = 0.26\n",
      "Iteration 1, training loss: 235522.05, validation loss = 504.90, time = 16.59\n",
      "Iteration 2, training loss: 214614.64, validation loss = 515.43, time = 16.41\n",
      "Iteration 3, training loss: 205248.79, validation loss = 474.89, time = 16.43\n",
      "Iteration 4, training loss: 199781.70, validation loss = 452.45, time = 17.70\n",
      "Iteration 5, training loss: 196144.97, validation loss = 436.49, time = 17.08\n",
      "Iteration 6, training loss: 193547.23, validation loss = 425.16, time = 16.20\n",
      "Iteration 7, training loss: 191597.89, validation loss = 418.99, time = 16.25\n",
      "Iteration 8, training loss: 190061.34, validation loss = 415.77, time = 15.67\n",
      "Iteration 9, training loss: 188813.93, validation loss = 413.78, time = 15.48\n",
      "Iteration 10, training loss: 187763.58, validation loss = 413.46, time = 15.78\n",
      "Iteration 11, training loss: 186854.34, validation loss = 415.16, time = 15.74\n",
      "Iteration 12, training loss: 186066.08, validation loss = 421.16, time = 15.54\n",
      "Iteration 13, training loss: 185383.89, validation loss = 433.37, time = 17.12\n",
      "Iteration 14, training loss: 184772.24, validation loss = 450.55, time = 16.26\n",
      "Iteration 15, training loss: 184216.57, validation loss = 469.79, time = 16.27\n",
      "Iteration 16, training loss: 183713.69, validation loss = 488.63, time = 16.25\n",
      "Iteration 17, training loss: 183251.43, validation loss = 507.51, time = 19.78\n",
      "Iteration 18, training loss: 182829.81, validation loss = 528.88, time = 17.14\n",
      "Iteration 19, training loss: 182451.71, validation loss = 552.67, time = 15.49\n",
      "Iteration 20, training loss: 182108.23, validation loss = 582.03, time = 15.48\n",
      "Converged after 10 iterations, on average 17.25 per iteration\n"
     ]
    }
   ],
   "source": [
    "Q_a_2, P_a_2, val_l_a_2, tr_l_a_2, conv_a_2 = latent_factor_alternating_optimization(M_shifted, nonzero_indices, \n",
    "                                                                           k=6, val_idx=val_idx,\n",
    "                                                                           val_values=val_values_shifted, \n",
    "                                                                           reg_lambda=0.1, init='random',\n",
    "                                                                           max_steps=100, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the validation and training losses over (training) time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24e16e27ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(val_l_a_2[:])\n",
    "plt.title('Alternating optimization validation loss, k=6')\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('Validation loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(tr_l_a_2[:])\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('Training loss')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latent factorization using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use gradient descent to factorize our ratings matrix. We will try both (mini-) batch and stochastic gradient descent. You can use the following equations for your implementation.\n",
    "\n",
    "Recall that the objective function (loss) we wanted to optimize was:\n",
    "$$\n",
    "\\mathcal{L} = \\min_{P, Q} \\sum_{(x, i) \\in W} (r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)^2 + \\lambda_1\\sum_x{\\left\\lVert \\mathbf{p}_x  \\right\\rVert}^2 + \\lambda_2\\sum_i {\\left\\lVert\\mathbf{q}_i  \\right\\rVert}^2\n",
    "$$\n",
    "\n",
    "where $W$ is the set of $(x, i)$ pairs for which $r_{xi}$ is known (in this case our known play counts). Here we have also introduced two regularization terms to help us with overfitting where $\\lambda_1$ and $\\lambda_2$ are hyper-parameters that control the strength of the regularization.\n",
    "\n",
    "Naturally optimizing with gradient descent involves computing the gradient of the loss function $\\mathcal{L}$ w.r.t. to the parameters. To help you solve the task we provide the following:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial ((r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)^2)}{\\partial \\mathbf{p}_x} = -2(r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)\\mathbf{q}_i\\;, ~~~\n",
    "\\frac{\\partial ((r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)^2)}{\\partial \\mathbf{q}_i} = -2(r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)\\mathbf{p}_x \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial(\\lambda_1{\\left\\lVert \\mathbf{p}_x \\right\\rVert}^2)}{\\partial \\mathbf{p}_x} = 2 \\lambda_1 \\mathbf{p_x} \\;, ~~~\n",
    "\\frac{\\partial(\\lambda_2{\\left\\lVert \\mathbf{q}_i \\right\\rVert}^2)}{\\partial \\mathbf{q}_i} = 2 \\lambda_2 \\mathbf{q_i}\n",
    "$$\n",
    "\n",
    "**Hint**: You have to carefully consider how to combine the given partial gradients depending\n",
    "on which variants of gradient descent you are using.  \n",
    "**Hint 2**: It may be useful to scale the updates to $P$ and $Q$ by $\\frac{1}{batch\\_size}$ (in the case of full-sweep updates, this would be $\\frac{1}{n\\_users}$ for $Q$ and $\\frac{1}{n\\_restaurants}$ for $P$).\n",
    "\n",
    "\n",
    "For each of the gradients descent variants you try report and compare the following:\n",
    "* How many iterations do you need for convergence.\n",
    "* Plot the loss (y axis) for each iteration (x axis).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_factor_gradient_descent(M, non_zero_idx, k, val_idx, val_values, \n",
    "                                   reg_lambda, learning_rate, batch_size=-1,\n",
    "                                   max_steps=50000, init='random',\n",
    "                                   log_every=1000, patience=20,\n",
    "                                   eval_every=50):\n",
    "    \"\"\"\n",
    "    Perform matrix factorization using gradient descent. Training is done via patience,\n",
    "    i.e. we stop training after we observe no improvement on the validation loss for a certain\n",
    "    amount of training steps. We then return the best values for Q and P oberved during training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    M                 : sp.spmatrix, shape [N, D]\n",
    "                        The input matrix to be factorized.\n",
    "                      \n",
    "    non_zero_idx      : np.array, shape [nnz, 2]\n",
    "                        The indices of the non-zero entries of the un-shifted matrix to be factorized. \n",
    "                        nnz refers to the number of non-zero entries. Note that this may be different\n",
    "                        from the number of non-zero entries in the input matrix M, e.g. in the case\n",
    "                        that all ratings by a user have the same value.\n",
    "    \n",
    "    k                 : int\n",
    "                        The latent factor dimension.\n",
    "    \n",
    "    val_idx           : tuple, shape [2, n_validation]\n",
    "                        Tuple of the validation set indices.\n",
    "                        n_validation refers to the size of the validation set.\n",
    "                      \n",
    "    val_values        : np.array, shape [n_validation, ]\n",
    "                        The values in the validation set.\n",
    "                      \n",
    "    reg_lambda        : float\n",
    "                        The regularization strength.\n",
    "\n",
    "    learning_rate     : float\n",
    "                        Step size of the gradient descent updates.\n",
    "                        \n",
    "    batch_size        : int, optional, default: -1\n",
    "                        (Mini-) batch size. -1 means we perform standard full-sweep gradient descent.\n",
    "                        If the batch size is >0, use mini batches of this given size.\n",
    "                        \n",
    "    max_steps         : int, optional, default: 100\n",
    "                        Maximum number of training steps. Note that we will stop early if we observe\n",
    "                        no improvement on the validation error for a specified number of steps\n",
    "                        (see \"patience\" for details).\n",
    "                      \n",
    "    init              : str in ['random', 'svd'], default 'random'\n",
    "                        The initialization strategy for P and Q. See function initialize_Q_P for details.\n",
    "    \n",
    "    log_every         : int, optional, default: 1\n",
    "                        Log the training status every X iterations.\n",
    "                    \n",
    "    patience          : int, optional, default: 10\n",
    "                        Stop training after we observe no improvement of the validation loss for X evaluation\n",
    "                        iterations (see eval_every for details). After we stop training, we restore the best \n",
    "                        observed values for Q and P (based on the validation loss) and return them.\n",
    "                      \n",
    "    eval_every        : int, optional, default: 1\n",
    "                        Evaluate the training and validation loss every X steps. If we observe no improvement\n",
    "                        of the validation error, we decrease our patience by 1, else we reset it to *patience*.\n",
    "                        \n",
    "    Returns\n",
    "    -------\n",
    "    best_Q            : np.array, shape [N, k]\n",
    "                        Best value for Q (based on validation loss) observed during training\n",
    "                      \n",
    "    best_P            : np.array, shape [k, D]\n",
    "                        Best value for P (based on validation loss) observed during training\n",
    "                      \n",
    "    validation_losses : list of floats\n",
    "                        Validation loss for every evaluation iteration, can be used for plotting the validation\n",
    "                        loss over time.\n",
    "                        \n",
    "    train_losses      : list of floats\n",
    "                        Training loss for every evaluation iteration, can be used for plotting the training\n",
    "                        loss over time.                     \n",
    "    \n",
    "    converged_after   : int\n",
    "                        it - patience*eval_every, where it is the iteration in which patience hits 0,\n",
    "                        or -1 if we hit max_steps before converging. \n",
    "\n",
    "    \"\"\"\n",
    "       \n",
    "    Q, P = initialize_Q_P(M, k, init)\n",
    "    \n",
    "    N = M.shape[0]\n",
    "    D = P.shape[1]\n",
    "    \n",
    "    converged_after = 0        \n",
    "    step_array = np.arange(batch_size, N, batch_size)\n",
    "    best_Q = Q\n",
    "    best_P = P\n",
    "    P_new = np.zeros(P.shape)\n",
    "    Q_new = np.zeros(Q.shape)\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    curr_patience = patience\n",
    "    zero_time = time.time()\n",
    "    start_time = time.time()\n",
    "    best_val_loss = 0\n",
    "    \n",
    "    step_array_N = []\n",
    "    step_array_D= []\n",
    "    lowest_border = 0\n",
    "    batch_size_N = N\n",
    "    batch_size_D = D\n",
    "    if batch_size > 0:\n",
    "        step_array_N = np.arange(batch_size, N, batch_size)\n",
    "        step_array_D = np.arange(batch_size, D, batch_size)\n",
    "        batch_size_N = batch_size\n",
    "        batch_size_D = batch_size\n",
    "    if N not in step_array_N:\n",
    "        step_array_N = np.append(step_array_N, N) \n",
    "    if D not in step_array_D:\n",
    "        step_array_D = np.append(step_array_D, D)\n",
    "    len_d = len(step_array_D)\n",
    "    len_n = len(step_array_N)\n",
    "    mult = np.array(nonzero_indices).shape[1]\n",
    "    for iteration in range(max_steps):        \n",
    "        \n",
    "        if iteration % eval_every == 0:\n",
    "            QP = Q.dot(P)\n",
    "            train_losses.append(np.sum(np.square(M[non_zero_idx] - QP[non_zero_idx])))\n",
    "            validation_losses.append(np.sum(np.square(val_values - QP[val_idx])))\n",
    "            if iteration == 0 or best_val_loss > validation_losses[-1]:\n",
    "                best_Q = Q\n",
    "                best_P = P\n",
    "                best_val_loss = validation_losses[-1]\n",
    "                curr_patience = patience\n",
    "                converged_after = iteration \n",
    "            else:\n",
    "                curr_patience -= 1\n",
    "      \n",
    "        if iteration % log_every == 0:\n",
    "            print(\"Iteration {:}, training loss: {:.2f}, validation loss = {:.2f}, time = {:.2f}\".\n",
    "                  format(iteration, train_losses[-1], validation_losses[-1], time.time()-start_time))\n",
    "            start_time = time.time() \n",
    "            \n",
    "        if batch_size == -1:   \n",
    "            diff = (M - Q.dot(P))\n",
    "            zero_M = np.zeros(M.shape)\n",
    "            zero_M[non_zero_idx] = diff[non_zero_idx]\n",
    "            P_new = P + 2 * learning_rate *  (np.dot(Q.T, zero_M) - reg_lambda * P)*N/mult\n",
    "            Q_new = Q + 2 * learning_rate *  (np.dot(zero_M, P.T) - reg_lambda * Q)*D/mult\n",
    "            P = P_new\n",
    "            Q = Q_new\n",
    "        else:\n",
    "            rand_items = np.random.choice(mult, batch_size, replace=False)\n",
    "            for element in rand_items:\n",
    "                row = non_zero_idx[0][element]\n",
    "                col = non_zero_idx[1][element]\n",
    "                diff = M[row, col] - Q[row,:].dot(P[:,col])\n",
    "                P_new[:,col] = P[:,col]  +  2 * learning_rate *  (Q[row,:].T * diff / batch_size - reg_lambda * P[:,col])\n",
    "                Q_new[row,:] = Q [row,:] +  2 * learning_rate *  (diff * P[:,col].T / batch_size - reg_lambda * Q[row,:])\n",
    "\n",
    "                P[:,col] = P_new[:,col]\n",
    "                Q[row,:]  = Q_new[row,:] \n",
    "        if curr_patience == 0 or max_steps==iteration:\n",
    "            break\n",
    "    print(\"Converged after {:} iterations, on average {:.2f} per iteration\".\n",
    "          format(converged_after, (time.time() - zero_time)/iteration))\n",
    "            \n",
    "    return best_Q, best_P, validation_losses, train_losses, converged_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the latent factor model with alternating optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Learn the optimal $P$ and $Q$ using standard gradient descent. That is, during each iteration you have to use all of the training examples and update $Q$ and $P$ for all users and songs at once. Try the algorithm with $k=30$, $\\lambda=1$, and learning rate of 0.1. Initialize $Q$ and $P$ with SVD.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss: 335340.78, validation loss = 213.70, time = 0.23\n",
      "Iteration 20, training loss: 282253.53, validation loss = 207.33, time = 21.18\n",
      "Iteration 40, training loss: 193022.47, validation loss = 216.51, time = 21.34\n",
      "Iteration 60, training loss: 118536.97, validation loss = 233.08, time = 19.65\n",
      "Iteration 80, training loss: 75682.40, validation loss = 243.33, time = 19.67\n",
      "Iteration 100, training loss: 53350.07, validation loss = 251.36, time = 19.67\n",
      "Iteration 120, training loss: 41225.36, validation loss = 258.40, time = 19.28\n",
      "Iteration 140, training loss: 34113.94, validation loss = 265.13, time = 19.09\n",
      "Iteration 160, training loss: 29602.46, validation loss = 271.81, time = 20.17\n",
      "Iteration 180, training loss: 26536.94, validation loss = 278.47, time = 19.14\n",
      "Iteration 200, training loss: 24332.19, validation loss = 285.05, time = 18.95\n",
      "Iteration 220, training loss: 22672.06, validation loss = 291.49, time = 19.19\n",
      "Iteration 240, training loss: 21375.18, validation loss = 297.74, time = 19.19\n",
      "Iteration 260, training loss: 20331.71, validation loss = 303.74, time = 20.72\n",
      "Iteration 280, training loss: 19471.91, validation loss = 309.48, time = 20.85\n",
      "Iteration 300, training loss: 18749.54, validation loss = 314.94, time = 19.23\n",
      "Iteration 320, training loss: 18132.84, validation loss = 320.10, time = 19.39\n",
      "Iteration 340, training loss: 17599.28, validation loss = 324.99, time = 19.30\n",
      "Iteration 360, training loss: 17132.42, validation loss = 329.59, time = 19.26\n",
      "Iteration 380, training loss: 16719.98, validation loss = 333.91, time = 19.26\n",
      "Iteration 400, training loss: 16352.58, validation loss = 337.97, time = 19.35\n",
      "Iteration 420, training loss: 16022.91, validation loss = 341.78, time = 19.47\n",
      "Converged after 20 iterations, on average 0.99 per iteration\n"
     ]
    }
   ],
   "source": [
    "Q_g_sweep, P_g_sweep, val_l_g_sweep, tr_l_g_sweep, conv_g_sweep =  latent_factor_gradient_descent(M_shifted, nonzero_indices, \n",
    "                                                                                                   k=30, val_idx=val_idx,\n",
    "                                                                                                   val_values=val_values_shifted, \n",
    "                                                                                                   reg_lambda=1, learning_rate=1e-1,\n",
    "                                                                                                   init='svd', batch_size=-1,\n",
    "                                                                                                   max_steps=10000, log_every=20, \n",
    "                                                                                                   eval_every=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the validation and training losses over (training) time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEWCAYAAACwtjr+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl8XVW5//HPN1PbtGnSNk3nNp3owFRKKYUClnlQf+AFlbkgigOoqFdF7vUig/fCvQqiIsoMMomCggoic2XsRCkd6EDnMWnTJumU8fn9sVfaQ8hw2uSck+F5v17ndfZee3rOzkme7LXXXktmhnPOOZdIaakOwDnnXMfnycY551zCebJxzjmXcJ5snHPOJZwnG+eccwnnycY551zCebJxbYakVZJOCdPXSbo3RXFMk7QuFcdOJUmXSXojZn6HpBHxrHsAx3pe0vQD3b6J/T4o6ebW3q9ruYxUB+DaB0nnA98BDgF2AiuBh4C7LAEPa5nZf7fGfiQVEsWaaWbVrbHPVJL0ILDOzP4z0ccysx6tsR9JPwFGmdnFMfs+szX27doPv7JxzZL0PeAO4P+A/kA/4GvAVCCrkW3Skxagc67N82TjmiQpF7gR+IaZ/cnMyi3ynpldZGYVYb0HJd0l6TlJO4ETJX1a0nuSyiStDf/hxu77EkmrJW2V9B/1lv1E0iMx81MkvSVpu6T3JU2LWfaapJskvSmpXNI/JeWHxTPC+/ZQLXRMA5+xW4h/m6RFwFH1lg+U9JSkYkkrJX0rZtlkSbPDZ9ws6baYZcfFxLxW0mWhvIukn0laE7b5raRuYdk0SeskfU9SkaSNki4Py64ELgJ+ED7LXxv4LL+V9LN6Zc9I+m6YvlbSR+E8LZL0ufr7iNnOJI0K030kPRs+50xgZL117wifsUzSHEnHh/IzgOuAL4aY34/5mX05TKdJ+s/wXSiS9HD43iGpMMQxPZyvLfW/K02R9BVJyyWVhPgHhnJJuj0cr1TSfEmHhGVnhXNTLmm9pH+P93iuCWbmL381+gLOAKqBjGbWexAoJbraSQO6AtOAQ8P8YcBm4Jyw/nhgB3AC0AW4LRznlLD8J8AjYXoQsBU4K+zr1DDfNyx/DfgIOAjoFuZvCcsKAWsqfuAW4F9Ab2AIsICoqopwvDnAfxFdxY0AVgCnh+VvA5eE6R7AlDA9FCgHLgAygT7AhLDsF8Cz4Xg5wF+B/wnLpoXzcGPY7ixgF9Ar5jzf3MRnOQFYCyjM9wJ2AwPD/OeBgeFzfZGoSnRAWHYZ8EbMvoyo+gvgCeBJoDtRVer6euteHD5jBvA9YBPQtf7PMmb914Avh+kvAcvDue0BPA38vt7P757wsz0cqADGNfE9vDlMnwRsASYSfcd+BcwIy04PP9c8QMC4mPOwETg+5vxNTPXvYUd4+ZWNa04+sMVi7nfE/Le+W9IJMes+Y2Zvmlmtme0xs9fM7IMwPx94HPhUWPc84G9mNsOiq6MfA7WNxHAx8JyZPRf29SIwm+gPcZ0HzGypme0m+qM4YT8+4xeAn5pZiZmtBX4Zs+wooqR2o5lVmtkKoj9854flVcAoSflmtsPM3gnlFwEvmdnjZlZlZlvNbJ4kAV8BvhOOVw78d8z+6vZ5Y9juOaKkPCbOz/Ivoj/Ox4f584C3zWwDgJn90cw2hPP4B2AZMLmpHSqqEj0X+C8z22lmC4ju1+1lZo+Ez1htZj8n+uMeb8wXAbeZ2Qoz2wH8CDhfUuw95RvMbLeZvQ+8T5R04tnv/WY2N3zHfgQco+g+XhVRoh9LlJgXm9nGsF0VMF5STzPbZmZz4/wcrgmebFxztgL5sb/4ZnasmeWFZbHfobWxG0o6WtKrofqplOg+T1311sDY9c1sZ9hfQ4YBnw8Jbruk7cBxwICYdTbFTO8i+g85Xh+LBVhd79gD6x37OqL7VgBXEF1RfShplqTPhPIhRFdb9fUFsoE5Mfv7Ryivs9U+3pgh7s9jZkZ0FXJBKLoQeLRuuaRLJc2LOfYh7PuZNKYv0RVLY+eIUO23OFRJbQdy49hvnYH19rc6HK9fTNmB/Hw/tt+QyLYCg8zsFeDXwJ3AZkl3S+oZVj2X6B+Z1ZJeVwNVr27/ebJxzXmbqNri7DjWrd8q7TGi6qIhZpYL/JaoygKiqoohdStKyiaqhmnIWqJqlbyYV3czu+UAYmrIx2IhqgKLPfbKesfOMbOzAMxsmZldABQAtwJ/ktQ9bPex+xrBFqJqrYNj9pdr8bf8iufzPA6cJ2kYcDTwFECYvwe4GugT/mFYwL6fSWOKiar2GjxH4f7MD4muEHuF/ZbG7Le5mDcQJfXYfVcTVbu2xMf2G34ufYiqADGzX5rZkcDBRP8wfD+UzzKzs4l+pn8hulJ2LeTJxjXJzLYDNwC/kXSepB7hhu4Eovr7puQAJWa2R9Jkov+y6/wJ+Ey4iZ5FdI+ise/jI8BnJZ0uKV1S13AjfXAcH6GYqHquwedFgieBH0nqFfb5zZhlM4EyST9U1JAgXdIhko4CkHSxpL5mVgtsD9vUEF1NnCLpC5Iywg32CWG9e4DbJRWEfQySdHocnwWiP8BNfRbM7L3wue8FXgg/Q4h+XhaWoajhwSHNHdDMaojuo/xEUrak8UDsMzI5RMmhGMiQ9F9Az5jlm4FCSY39fB8HviNpuKQeRNWKf7CWN1V/DLhc0gRJXcJ+3zWzVZKOClfemUT3rfYANZKyJF0kKdfMqoAyop+nayFPNq5ZZva/wHeBHwBFRH88fkf03+xbTWz6DeBGSeVEN9j3/odoZguBq4j+IGwEtgENPkgZ7qOcTVR9VUx01fB94vj+mtku4KfAm6HqaEoDq91AVN2yEvgn8PuY7WuAzxLdA1pJdGVyL1E1EUQNKBZK2kHUPPz8cL9qDVFVzPeAEmAe++4z/JDohvg7ksqAl4j//sZ9RPcTtkv6SxPrPQ6cQnR+6z7LIuDnRFerm4kab7wZ53GvJqq62kR0E/6BmGUvAM8DS4nO4x4+XuX2x/C+VVJD9z/uJzrnM4jO8R4+nvAPiJm9THQv8Cmi79hI9t0b60mU9LeFmLcCda34LgFWhZ/N14juGboWqmux4pxzziWMX9k455xLOE82zjnnEs6TjXPOuYTzZOOccy7hvNfnID8/3woLC1MdhnPOtStz5szZYmZ9m1vPk01QWFjI7NmzUx2Gc861K5JWN7+WV6M555xLAk82zjnXidX1ypxoXo3mnHOdRPmeKpZuLufDTeUs2bTv/YVrTqB/bteEHtuTjXPOdTBVNbWsKN7Jh5vKWBKTWNZv3713nR5dMjioXw/OOnQANX5l45xzrjFmRlF5BYs2lLE4JrF8VLyDqpoogWSkiZF9e3DksF5cePRQxvbPYUz/HAbldSMaXik5PNk451w7UF1Ty4otO6PEsrGMRRvLWLShjK07K/euMyivG2P653Di2IK9SWVEfg+yMlJ/e96TjXPOtTE7Kqr5MCahLNoYXbVUVEeD2WZlpDGmXw6njOvH+IE9GT+wJ2P659Cza2aKI2+cJxvnnEuhbTsr+WB9KR+sL2XhhlIWbShj1dZde5f3ys7k4IG5XHrMsCixDMhlRN/uZKan/mplf3iycc65JCkJiWXB+lI+WBclmNib9kN7Z3PIoJ6cd+TgvYmlX88uSb23kiiebJxzLgG27qjYl1jWl7JgfdnHEkthn2yOGJrHpccM49BBuRw8KJfcbm23GqylPNk451wLle+p4oP1pcxbu533127ng3WlbCjds3f58PzuTBzWi+nHDuOQQbkcPLBjJ5aGeLJxzrn9UFVTy5JN5cxbu31vcllevIO6R1UK+2RzZGFvLh+UGyWWQT3b9I37ZPFk45xzjTAz1pTsCkmllPfXbWfB+tK9rcJ6d89iwpA8PnPYQCYMzeOwQbn06p6V4qjbpqQmG0ndgd1mVivpIGAs8LyZVSUzDueca8iOimrmrdnO3DXbmLtmG++v3c62XdGfp66ZaRw6KJdLpgzj8CF5TBiSx+BeyX0wsj1L9pXNDOB4Sb2Al4HZwBeBi5Ich3Ouk6u7apmzehtzVm9j7prtLNlURq2BBKMLenDa+P4cPiSPw4fkclC/nHbX3LgtSXaykZntknQF8Csz+19J7yU5BudcJ7Snqob560qZuyYkl9Xb9j59n9MlgwlD8zjtpNEcOawXE4bm+X2WVpb0ZCPpGKIrmStSFINzrhMoKtvDrFXbmL26hLlrtrNwfSnVtdFd/OH53Zk2poCJw/I4clgvRhfkkJ7m1WGJlOw/9NcAPwL+bGYLJY0AXk1yDM65DsbMWLV1F7NWljBzVQmzVpWwOjyF3zUzjcMH5/GVE0Zw5NBeHDE0jz49uqQ44s4nqcnGzF4HXgeQlAZsMbNvJTMG51z7V1NrfLipjFkrS5i1ahszV5VQXF4BRN27TCrszcVHD+Oo4b05eGBPv9fSBiS7NdpjwNeAGmAOkCvpNjP7v2TG4ZxrXyqqo/stM1dGVy1zVm2jvKIaiHo6njqyD0cN783kwt6M7NuDNK8Sa3OSXY023szKJF0EPAf8kCjpeLJxzu1VUV3De2u2886KrbyzYitz12ynMjzbMrqgB5+dMJDJhb05anhvBuV1S3G0Lh7JTjaZkjKBc4Bfm1mVpMQPEeeca9Mqqmt4f20p76zYytsfbWXumm1UVNciwcEDe3LJlGEcPbw3kwp709sfmmyXkp1sfgesAt4HZkgaBpQlOQbnXIpVVtcyf9123v5oK++s3Mqc1dvYUxUll3H9e3LxlGFMGdGHyYW9yc32JsgdQbIbCPwS+GVM0WpJJyYzBudc8lXX1PL+utK91WKzV21jd1UNAOMG9OSCyUOZMqIPRw/vTV62X7l0RMluIJALXA+cEIpeB24ESpMZh3MuscyM5UU7eGP5Ft5cvpV3V2zde0N/bP8cvnjUkL3JxfsS6xySXY12P7AA+EKYvwR4APi3JMfhnGtlG0t38+byrby1fAtvLN9CUWiKPKxPNp85fCDHjcpnyoje/oxLJ5XsZDPSzM6Nmb9B0rwkx+CcawWlu6t4Z8VW3ly+hTeXb+Gj4p0A9OmexTEj+3DcqHymjspnSO/sFEfq2oJkJ5vdko4zszcAJE0FdjezjXOuDaisrmXumm38a1kxbyzfygfrtlNr0C0znaNH9Ob8o4YydVQ+Y/vn+HMu7hOSnWy+DjwU7t0IKAEua24jSV2JeozuQhTzn8zseknDgSeA3sBc4BIzq5TUBXgYOBLYCnzRzFa1/sdxrmNbtWUn/1pWzOtLt/D2R1vYWVlDepqYMCSPq08cxdRR+RwxtBdZGf6EvmtaslujzQMOl9QzzMfb7LkCOMnMdoTndN6Q9DzwXeB2M3tC0m+JOve8K7xvM7NRks4HbiUaysA514QdFdW8tXwLM5YVM2PpFtaURP2LDe7VjbOPGMQJo/ty7Kg+3iOy229JSTaSvttIOQBmdltT25uZATvCbGZ4GXAScGEofwj4CVGyOTtMA/wJ+LUkhf0454LaWmPhhjJmLCvm9aXFzF29jepaIzsrnWNG9OGK44ZzwkF9KeyT7YOEuRZJ1pVNTkt3ICmdqGubUcCdwEfAdjOrDqusAwaF6UHAWgAzq5ZUCvQBttTb55XAlQBDhw5taYjOtQtbdlQwY2mUXN5YtmXvmC7jB/Tky8eP4ISD8jlyWC+6ZKSnOFLXkSQl2ZjZDa2wjxpggqQ84M/AuIZWC+8N/Qv2iasaM7sbuBtg0qRJftXjOqTaWmP++lJe/bCI15YUMX99KWaQ3yOLEw7qy/Gj8zl+dF/65niTZJc47W7gMjPbLuk1YAqQJykjXN0MBjaE1dYBQ4B1kjKAXKLGCM51Ctt2VjJjWTGvLYmuYEp2ViLBEUPy+M4pB3HimAIOHtjTW425pGkXyUZSX6AqJJpuwClEN/1fBc4japE2HXgmbPJsmH87LH/F79e4jqy21li0sYxXPyzi1SVFzFsbNUvu3T2LTx3Ul2lj+nLC6L7+tL5LmXaRbIABRE2m04E04Ekz+5ukRcATkm4G3gPuC+vfB/xe0nKiK5rzUxG0c4lUtqeKfy3dwqtLinhtSTFbdkRP7B8+OJdvnjSaE8cWcOigXB/u2LUJye4brQtwLlAYe2wzu7Gp7cxsPnBEA+UrgMkNlO8BPt/CcJ1rc1YU7+CVD4t4eXERs1aVUF1r5HbL5ISD+nLimL6ccFBf8r07GNcGJfvK5hmiTjfnED0745xrQmV1LbNXlfDyh0W88mERK7dEXcKM7Z/DlSeM4KSxBUwYkkeGD3vs2rhkJ5vBZnZGko/pXLuydUcFry0p5pUPi5ixtJjyimqy0tM4ZmQfLp9ayEljCxjcy/sbc+1LspPNW5IONbMPknxc59osM+PDTeWhemwz763djhn0zenCpw8bwEljC5g6Kp/uXdrLLVbnPinZ397jgMskrSSqRhNRBwGHJTkO51KqorqGd1eU8NLizby8uIj126P+aA8bnMu3Tx7NyWP7edNk16EkO9mcmeTjOddmbNtZyatLopv7ry8tZkdFNV0z0zhuVF++edIoThpbQEHPrqkO07mESHZHnKslHQ4cH4r+ZWbvJzMG55JpRfEOXl5cxIuLNzN7VQm1BgU5Xfjs4QM4ZVw/po7Kp2umdwvjOr5kN33+NvAV4OlQ9Iiku83sV8mMw7lEqak15q7ZxkuLNvPi4s2sCAOKjRvQk6tOHMUp4/px6KBcrx5znU6yq9GuAI42s50Akm4lesrfk41rt3ZWVDNjaTEvLt7Mqx8WsW1XFZnpYsqIPkw/ppCTx3nrMeeSnWwE1MTM19Bwp5nOtWmby/bw0uLNvLhoM28t30plTS253TI5aWwBp4zrxwkH5ZPjY744t1eyk80DwLuS/hzmz2FfFzPOtVlmxpLN5VH12KLNvL+uFIChvbO55JhhnDq+H5OG9fKHK51rRLIbCNwWemw+juiK5nIzey+ZMTgXr+qaWmauKuHFRZt5afFm1pZEzZMnDMnj+6eP4dTx/Rhd0MMHFXMuDskaqbOnmZVJ6g2sCq+6Zb3NzLv/d21C+Z4qZizdwouLNvHqkmJKd1eRlZHGcaPy+ca0UZzszZOdOyDJurJ5DPgMUZ9osV39K8yPSFIczn3CptI9vBjuv7zzUXT/pVd2JqeM68ep4/tx/Gh/et+5lkrWSJ2fCe/Dk3E855pS1z1MXfPk+eH+S2GfbKYfO4xTx/fnyGG9vGt+51pRsp+zednMTm6uzLnW1tD9Fym6//KDM8Zw2vh+jOzr91+cS5Rk3bPpCmQD+ZJ6sa+5c09gYDJicJ3PjrrnXxZt5pUPi/befzm+7v7LuAIKcvz+i3PJkKwrm68C1xAlljnsSzZlwJ1JisF1AkVl++6/1D3/Env/5YSD8snO8vsvziVbsu7Z3AHcIemb3jWNa01mxkfFO3hhYZRg5q3dDsCwcP/llHH9ONKff3Eu5ZL9nM2vJB0CjAe6xpQ/nMw4XPtWU2u8t2Yb/wwPWNaNXnm4P//iXJuV7AYC1wPTiJLNc0RDDrwBeLJxTdpTVcMby7bsvcG/dWclmenimJH5XHHccE4Z14/+uX7/xbm2KtmV1+cBhwPvmdnlkvoB9yY5BtdObN9VycuLi/jnok3MWLqF3VU15HTJYNrYAk4b349PjelLT+9/zLl2IdnJZreZ1UqqltQTKMIf6HQxissreGHhJv6xYBNvr9hKTa3Rv2dXzjtyMKeO78eUEX3IyvD7L861N8lONrMl5QH3ELVK2wHMTHIMro3ZsH03/1gQJZhZq0swg+H53bnyhBGccXB/Dhuc6/dfnGvnkt1A4Bth8reS/gH0NLP5yYzBtQ1rtu7i+QUbeX7Bpr0tyMb0y+FbJ43mzEP7M6ZfjicY5zqQZD3UObGpZWY2t5nthxA1IugP1AJ3m9kdkn5CNPJncVj1OjN7LmzzI6LB2mqAb5nZCy3+IK5FlheV8/wHm3h+wSYWbSwD4NBBuXz/9DGceUh/RvTtkeIInXOJkqwrm5+H967AJOB9ogc7DwPeJRpyoCnVwPfMbK6kHGCOpBfDstvN7GexK0saD5wPHEz0IOlLkg4ysxpcUn1UvINn3lvPcws2sbxoBwATh+bxH2eN44xD+jOkt49g6VxnkKyHOk8EkPQEcKWZfRDmDwH+PY7tNwIbw3S5pMXAoCY2ORt4wswqgJWSlgOTiYagdgm2fVclf52/kafmrGPe2u2kCSYP780lUw7m9IP7exNl5zqhZDcQGFuXaADMbIGkCfuzA0mFwBFEV0RTgaslXQrMJrr62UaUiN6J2WwdDSQnSVcCVwIMHTp0vz6I+7iqmlpeX1LMU3PX8fLiIiprahnTL4frzhrLORMG+RgwznVyyU42iyXdCzxCNI7NxcDieDeW1AN4CrgmDMZ2F3BT2NdNRNV1X2Jf32ux7BMFZncDdwNMmjTpE8td08yMhRvKeGruOp6dt4GtOyvp0z2Li6YM5dyJgzl4YE+/ye+cA5KfbC4Hvg58O8zPAO6KZ0NJmUSJ5lEzexrAzDbHLL8H+FuYXQcMidl8MLChRZG7vYrK9vCXeet5eu56PtxUTlZ6GiePK+DciYP51Ji+ZHo/ZM65epLd9HkPcHt4xU3Rv8f3AYvN7LaY8gHhfg7A54AFYfpZ4DFJtxE1EBiNP8/TInuqanhx0WaemruOGUuLqbVoLJibzjmEzx42gLzsrFSH6Jxrw5LV9PlJM/uCpA9ouDrrsGZ2MRW4BPhA0rxQdh1wQbjnY8AqoqEMMLOFkp4EFhG1ZLvKW6IdmNLdVTzyzmruf2MlW3dWMjC3K1+fNpLPHTGYUQXeVNk5Fx+ZJf5WRd0ViKRhDS03s9UJD6IZkyZNstmzZ6c6jDajuLyC+95YyaPvrKa8oppPHdSXrxw/gmNH9iHNh0t2zgWS5pjZpObWS1bT57pmyylPKq5pa0t28bsZH/Hk7HVU19Ry5qED+PqnRnLIoNxUh+aca8eSVY1WTgPVZ0StxszMeiYjDte4JZvKueu15fx1/kbSBOdOHMxXPzWS4fndUx2ac64DSNaVTU4yjuP235zV27jrteW8tLiI7Kx0Lj+2kC8fP8IfvHTOtaqUDMYuqYCPj9S5JhVxdFZmxoxlW/jNq8t5d2UJedmZXHPKaKYfU0iv7t6qzDnX+pI9Uuf/I3rwciDRWDbDiB7qPDiZcXRWZsbzCzbxm9eWs2B9Gf17duU/Pz2OCyYPpXuXlPzf4ZzrJJL9F+YmYArwkpkdIelE4IIkx9Aple2p4gd/nM8/Fm5ieH53bj33UM45YhBdMtJTHZpzrhNIdrKpMrOtktIkpZnZq5JuTXIMnc7CDaVc9ehc1m7bzXVnjeWK40aQ7s2XnXNJlOxksz30bzYDeFRSEdFDly5Bnpy1lh8/s4C87EyeuHIKRxX2TnVIzrlOKNnJ5mxgD/Ad4CIgF7gxyTF0Crsra/jxMwv405x1TB3VhzvOP4L8Hl1SHZZzrpNK1nM2vwYeM7O3YoofSsaxO6MVxTv4xqNzWbK5nG+dNIpvn3KQV5s551IqWVc2y4CfSxoA/AF43MzmNbONOwB/n7+RHz41n8x08cBlRzFtTEGqQ3LOuaQ91HkHcEfoG+184AFJXYHHiUbUXJqMODqyyupa/uf5xTzw5iqOGJrHnRdOZGBet1SH5ZxzQPKHGFgN3ArcKukI4H7gesDb37bAhu27ueqxuby3ZjuXTy3kR2eOIyvDx5RxzrUdyX6oMxM4g+jq5mTgdeCGZMbQ0by+tJhrnniPqhrjzgsn8unDBqQ6JOec+4RkNRA4lejhzU8TDWL2BHClme1MxvE7oppa446Xl/GrV5Yxpl8Ov7loIiP6+vgyzrm2KVlXNtcBjwH/bmYlSTpmh1VcXsF3/jCPN5Zv4bwjB3PT2YfQLctrIp1zbVeyGgicmIzjdHSLN5bx8Nur+PN76zGD/z33ML5w1JBUh+Wcc83y3hdbaNaqErLS0zh0UG5CRrCsqqnlnws389Bbq5i5qoSumWmcM2EQXz5+OKMKfOQG51z74Mmmhf7vhSXMXFlCfo8uTBvTl5PHFnDc6Hxyuma2aL/F5RU8MXMNj767hk1lexjSuxvXnTWWL0waQl62DwPgnGtfZNbQAJqdz6RJk2z27Nn7vV3JzkpeX1rEy4uLmLG0mLI91WSmi6MKe3PS2AJOGluwXzfu31uzjYfeWsXfP9hIVY1x/Oh8Lju2kGljCrwXAOdcmyNpjplNanY9TzaRA002saprapmzehuvLCni1Q+LWLp5BwCFfbI5cWwBJ4/tx+ThvT/xDMyeqhr+Pn8jD7+9ivfXldKjSwbnHTmYS44ZxkhvYeaca8M82eyn1kg29a0t2cWrS4p45cMi3vpoK5XVtXTPSue40fmcNLaAQwfl8fcPNvDEzLVs3VnJqIIeTD9mGJ+bOJgePpiZc64d8GSznxKRbGLtrqzhrY+28PKH0VXPxtI9AKQJThnXj+nHFnLsyD5IXlXmnGs/4k02/u9zknTLSufkcf04eVw/zIwPN5Xz/trtTB2Vz5De2akOzznnEqpddKAlaYikVyUtlrRQ0rdDeW9JL0paFt57hXJJ+qWk5ZLmS5qY2k/wcZIYN6An508e6onGOdcptItkQzSa5/fMbBwwBbhK0njgWuBlMxsNvBzmAc4ERofXlcBdyQ/ZOedcnXaRbMxso5nNDdPlwGJgENHIn3WDsD0EnBOmzwYetsg7QF4YS8c551wKtLt7NpIKgSOAd4F+ZrYRooQkqW6ksEHA2pjN1oWyjY3td86cOVskrT7AsPKBLQe4bWfh56hpfn6a5ueneak6R8PiWaldJRtJPYCngGvMrKyJllsNLfhEsztJVxJVswH8h5ndfYBxzY6nNUZn5ueoaX5+mubnp3lt/Ry1m2QTxsJ5CnjUzJ4OxZslDQhXNQOAolC+DojtoXIwsKH+PkNyOaAE45xzLn7t4p6NokuY+4DFZnZbzKJngelhejrwTEz5paFV2hSgtK66zTnnXPK1lyubqcAlwAeS5oWy64BbgCclXQGsAT4flj0HnAUsB3YBlyc4Pr86ap6fo6b5+Wman5/ArvLFAAAdTUlEQVTmtelz5D0IOOecS7h2UY3mnHOuffNk45xzLuE82bSQpDMkLQld41zb/Badi6RVkj6QNE9S4no6bUck3S+pSNKCmLIGu17qjBo5Pz+RtD58j+ZJOiuVMabS/nbf1VZ4smkBSenAnUTd44wHLgjd6LiPO9HMJrTlZwCS7EHgjHpljXW91Bk9yCfPD8Dt4Xs0wcyeS3JMbcn+dt/VJniyaZnJwHIzW2FmlcATRF3lONcoM5sBlNQrbqzrpU6nkfPjggPovqtN8GTTMo11i+P2MeCfkuaEHhtcwz7W9RJQ0Mz6ndHVoRf3+9taFVGqNNV9F23sO+TJpmXi6hank5tqZhOJqhqvknRCqgNy7dJdwEhgAlEfhz9PbTipV7/7rlTH0xxPNi0TV7c4nZmZbQjvRcCfiaoe3SdtruuZvF7XSw4ws81mVmNmtcA9dPLvUVPdd4Xlbe475MmmZWYBoyUNl5QFnE/UVY4DJHWXlFM3DZwGLGh6q06rsa6XHHv/eNb5HJ34e3QA3Xe1Cd6DQAuFJpi/ANKB+83spykOqc2QNILoagairpEe8/MDkh4HphF1Cb8ZuB74C/AkMJTQ9ZKZdcqb5I2cn2lEVWgGrAK+2ln7O5R0HPAv4AOgNhRfR3Tfps1+hzzZOOecSzivRnPOOZdwnmycc84lnCcb55xzCddexrNJuPz8fCssLEx1GM45167MmTNni5n1bW49TzZBYWEhs2d7P5HOObc/JK2OZz2vRnPOOZdwnmxaaHnRDt5bsy3VYTjnXJvmyaYFzIxrn5rPpffPZMH60lSH45xzbZYnmxaQxB0XHEHPrplcfN+7LN7Y5vvCc865lPBk00KD8rrx+Fem0DUjnYvvfZflReWpDsk559ocTzatYGifbB77ytFI4sJ73mXllp2pDsk559oUTzatZETfHjz2laOprjUuvOcd1pbsSnVIzjnXZniyaUUH9cvhkSuOZldlDRfc8w4btu9OdUjOOdcmeLJpZeMH9uT3V0ymdFcVF97zDpvL9qQ6JOecS7mEJRtJXSXNlPS+pIWSbgjlwyW9K2mZpD+EQceQ1CXMLw/LC2P29aNQvkTS6THlZ4Sy5ZKujSlv8BjJctjgPB780mSKyyu48J532LKjIpmHd865NieRVzYVwElmdjjRoEdnSJoC3ArcbmajgW3AFWH9K4BtZjYKuD2sh6TxRCNgHgycAfxGUrqkdOBOorHtxwMXhHVp4hhJc+SwXtx/2VGs376bi+99l207K5MdgnPOtRkJSzYW2RFmM8PLgJOAP4Xyh4BzwvTZYZ6w/OQw/OnZwBNmVmFmK4HlROOPTwaWm9kKM6sEngDODts0doykOnpEH+699ChWbNnJJfe/S+nuqlSE4ZxzKZfQezbhCmQeUAS8CHwEbDez6rDKOmBQmB4ErAUIy0uBPrHl9bZprLxPE8eoH9+VkmZLml1cXNySj9qo40bn87uLj2TJpnKm3z+T8j2ecJxznU9Ck42Z1ZjZBGAw0ZXIuIZWC+9qZFlrlTcU391mNsnMJvXt22wP2QfsxLEF3HnhRBasL+VLD85iV2V18xs551wHkpTWaGa2HXgNmALkSaob2mAwsCFMrwOGAITluUBJbHm9bRor39LEMVLmtIP784vzJzBn9Ta+/NBs9lTVpDok55xLmkS2RusrKS9MdwNOARYDrwLnhdWmA8+E6WfDPGH5K2Zmofz80FptODAamAnMAkaHlmdZRI0Ing3bNHaMlPrMYQP5+RcO5+0VW7ny93OoqPaE45zrHBJ5ZTMAeFXSfKLE8KKZ/Q34IfBdScuJ7q/cF9a/D+gTyr8LXAtgZguBJ4FFwD+Aq0L1XDVwNfACURJ7MqxLE8dIuc8dMZhb/u1QZiwt5qpH51JVU5vqkJxzLuEUXQjEuXLU0ivbzDpc51+TJk2yZI7U+fu3V/HjZxbywzPG8vVpI5N2XOeca02S5pjZpObWa/bKRtLDknpKygYWAislfbc1guzMLjmmkFPGFfDrV5ZRVO69DDjnOrZ4qtEONbMyomdV/kl0w/2yRAbVWfzHp8dTWVPL//1jSapDcc65hIon2WSFll1nA38JD1D6jYZWMDy/O1+aOpw/zlnH+2u3pzoc55xLmHiSzb3AGqAX8LqkocCOpjdx8br6pFHk98jixr8tYn/unznnXHvSbLIxs9vNbKCZnRaaFa8l6g7GtYKcrpl8//QxzFm9jWffT/njQM45lxDxNBC4WlLPMP074F3g+EQH1pmcd+QQDhnUk1ue/9B7F3DOdUjxVKNdaWZlkk4j6mPs68D/JjasziU9TVz/2YPZWLqH376+ItXhOOdcq4sn2dTdSDgTeMDM5sS5ndsPRxX25rOHD+R3r3/Eum0+pLRzrmOJJ2m8L+k54LPA85J60EjHlq5lrj1zLBLc8vyHqQ7FOedaVTzJ5nLgJ8BkM9sFdCUFg5F1BoPyuvHVE0byt/kbmbmyJNXhOOdcq4mnNVoNkA/8QNItwFFm9l7CI+ukvvapkQzI7coNf11ITa1fQDrnOoZ4WqP9FPgBsCK8vi/p5kQH1ll1y0rnR2eNY+GGMv44e23zGzjnXDsQTzXaZ4FTwkBjdwOnAf8vsWF1bp89bACThvXiZ/9cQpmP7Omc6wDibVWW08i0SwApagq9dWclv35learDcc65Fosn2fwvMFfSvZLuA2YDtyY2LHfo4Fw+f+RgHnhzJSuKvXcg51z7Fk8DgUeA44DnwusEM3s00YE5+PfTx9AlI52f/n1xqkNxzrkWaTTZSDqs7kU02uVyYBnRaJqHJSvAzqwgpyvfPGkUL39YxOtLi1MdjnPOHbCMJpbd2cQyA05o5VhcAy6bWshjM9dw098Wcey3jycz3TtvcM61P40mGzPzzjbbgC4Z6fznp8fzlYdn88g7q7l86vBUh+Scc/stYf8mSxoi6VVJiyUtlPTtUN5b0ouSloX3XqFckn4pabmk+ZImxuxrelh/maTpMeVHSvogbPNLSWrqGO3VKeMKOH50Pre/uJSSnZWpDsc55/ZbIutkqoHvmdk4YApwlaTxwLXAy2Y2Gng5zEPU0efo8LoSuAuixAFcDxwNTAauj0ked4V167Y7I5Q3dox2SRI//sx4dlbWcPuLS1MdjnPO7beEJRsz22hmc8N0ObCYaIiCs4GHwmoPAeeE6bOBhy3yDpAnaQBwOvCimZWY2TbgReCMsKynmb0dBnV7uN6+GjpGu3VQvxwumTKMR99dzYebylIdjnPO7Zd4uqs5rIHXMElxJypJhcARRAOv9TOzjRAlJKAgrDaIaBTQOutCWVPl6xoop4ljtGvXnDKant0yueFZH0LaOde+xJMw7gPmEF05/J7ooc4/A8skndzcxmFIgqeAa8ysqX/J1UCZHUB53CRdKWm2pNnFxW2/aXFedhbfPfUg3l6xlRcWbk51OM45F7d4ks0y4Egzm2BmhwNHAvOIqrd+3tSGkjKJEs2jZvZ0KN4cqsAI70WhfB0wJGbzwcCGZsoHN1De1DE+JvT3NsnMJvXt27epj9JmXDh5KGP65XDjXxdSutv7TXPOtQ/xJJtxZja/bsbMPgAmmlmTnXaFlmH3AYvN7LaYRc8CdS3KpgPPxJRfGlqlTQFKQxXYC8BpknqFhgGnAS+EZeWSpoRjXVpvXw0do93LSE/j1vMOY3N5BTc8uzDV4TjnXFziSTYfSfqVpKnh9UtguaQuRC3OGjMVuAQ4SdK88DoLuAU4VdIy4NQwD1FXOCuIeiq4B/gGgJmVADcBs8LrxlAG8HXg3rDNR8DzobyxY3QIE4bkcdWJo3j6vfU8/8HGVIfjnHPNUnM3miVlA98k6h9NwBvAr4A9QA8zK010kMkwadIkmz17dqrDiFtVTS3n3vUWa0t28cI1J1DQs2uqQ3LOdUKS5pjZpObWi6cjzl1mdquZfdbMPmNmt5jZTjOr6SiJpj3KTE/jti9MYFdlDT98ar63TnPOtWnxNH2eIul5SYskLa17JSM417RRBT249syxvLqkmMdn+qiezrm2q6mOOOs8QDQs9BygJrHhuP01/ZhCXlq8mZv/voipo/owrE/3VIfknHOfEE8DgTIz+6uZbTCzzXWvhEfm4pKWJv7vvMNJTxPfffJ9amq9Os051/bEk2xekfQ/ko6qN8aNayMG5nXjprMPYc7qbfz29Y9SHY5zzn1CPNVox9V7Bx/Pps05e8JAXly0mV+8tJRpY/py8MDcVIfknHN7NZtsfFyb9kESN59zCLNWlfCdP8zj2auPo2tmeqrDcs45oOlhoS8I799q6JW8EF28enXP4tbzDmPp5h38/J9LUh2Oc87t1dQ9m7oxY/o28nJt0IljCrjo6KHc+8ZK3lmxNdXhOOccEEcPAp1Fe+tBoCm7Kqs5645/UVVj/OOa48npmpnqkJxzHVSr9SAgKV/SDyT9RtLdda/WCdMlQnZWBrd9cQIbS3dzw18XpToc55yLq+nzM0A/oj7RXo55uTZs4tBefGPaKP40Zx0vLNyU6nCcc51cPE2fu5vZ9xIeiWt13zp5NK8uKeJHT3/AxKG96JvTJdUhOec6qXiubJ6XdFrCI3GtLisjjV98cQI7Kqr50dPeWadzLnXiSTZfA/4haYekEknbJJU0u5VrE0b3y+EHp4/hpcVFPDnbO+t0zqVGPMkmH8gEcomaPOfjTZ/blS9NHc4xI/pw418XsWbrrlSH45zrhJp6qHN0mDy4kZdrJ9LSxM++cDhpEt/74zzvrNM5l3RNXdlcG97vbOD16wTH5VrZoLxu3HD2wcxatY2v/n42OyqaGtHbOedaV6Ot0czsivDufaN1EP82cTDle6q58W+LOPc3b3Hv9EkM6Z2d6rCcc51APPdskDRW0r9JurDulejAXGJMP7aQBy8/ig2luznnzjeZtcrbejjnEi+eHgT+E7gb+C1wJvAL4Lw4trtfUpGkBTFlvSW9KGlZeO8VyiXpl5KWS5ovaWLMNtPD+sskTY8pP1LSB2GbX0pSU8dw+xw/ui9/uWoqPbtlcuE97/BHb6XmnEuweK5svgicCGw0s0uAw4nvYdAHgTPqlV0LvGxmo4l6Iai7L3QmMDq8rgTugihxANcDRwOTgetjksddYd267c5o5hguxsi+PfjLN6YyeXhvvv+n+fz3c4u94YBzLmHiSTa7zawGqJaUA2wCRjS3kZnNAOrX0ZwNPBSmHwLOiSl/2CLvAHmSBgCnAy+aWYmZbQNeBM4Iy3qa2dsWPan4cL19NXQMV09udiYPXj6ZS6YM4+4ZK7jy4dmU76lKdVjOuQ4onmTznqQ84H5gNjATmHuAx+tnZhsBwntBKB8ExNblrAtlTZWva6C8qWN8gqQrJc2WNLu4uPgAP1L7lpmexk3nHMJNZx/Ma0uLOfeut/xZHOdcq2sy2YT7ID8xs+1mdifwaeCrZnZpK8ehBsrsAMr3i5ndbWaTzGxS376d+znVS44p5OEvTWZzWQXn/OZNZq70hgPOudbTZLIJVVR/i5lfbmYHelUDsDlUgRHei0L5OmBIzHqDgQ3NlA9uoLypY7hmTB2Vz1+umkpet0wuuvcdnpzlDQecc60jnmq0mbGtw1roWaCuRdl0ouEL6sovDa3SpgCloQrsBeA0Sb1Cw4DTgBfCsnJJU8LV16X19tXQMVwchud358/fmMqUEX34wVPzuflvi7zhgHOuxZrqrqauxdlxRAlniaS5kt6T1OzVjaTHgbeBMZLWSboCuAU4VdIy4NQwD/AcsAJYDtwDfAPAzEqAm4BZ4XVjKAP4OnBv2OYj4PlQ3tgxXJxyszN54LKjuOzYQu59YyVffmiWNxxwzrVIo8NCS5prZhMljWxouZl9lNDIkqwjDQvdmh59dzXXP7OQ4fndufvSSQzP757qkJxzbUi8w0I39byMoOMlFbd/Ljp6GMPzu/P1R+Zyym2vc+Yh/bl86nAmDs0jPEfrnHPNairZ9JX03cYWmtltCYjHtUHHjsznH9ccz/1vrOSJWWv52/yNHD44l8unDuesQweQlRFXr0fOuU6sqWq0jURP6Tf476uZ3ZDAuJLOq9His7OimqfnruOBt1axongnBTlduHjKMC48eij5PXzYaec6m3ir0Zq9Z9PqkbVRnmz2T22tMWNZMQ+8uYrXlxaTlZHG/zt8IJdPLeTggbmpDs85lyStds/GuYakpYlpYwqYNqaA5UU7eOitVfxpzjr+NGcdk4f35ktTCzl1fH/S0/xr5Jxr+sqmd0wz4w7Pr2xarnR3FU/OWstDb69i3bbdDMrrxvRjh/HFSUPJzc5MdXjOuQRocTVaZ+PJpvXU1BovLtrMA2+u5N2VJXTNTOPIYb2YNKw3RxX25oiheXTvEk/H4c65tq41qtGcOyDpaeKMQ/pzxiH9WbihlD/OXsfMlSX88pVlmEXLDx7YMySfXkwq7E3fHG9c4FxH5lc2gV/ZJF7ZnireW7Od2atKmLmyhHlrt1NRXQtE3eRMGtaLowp7M6mwF8Pzu/tzPM61A16Ntp882SRfZXUtCzaUhuSzjdmrS9i+K+oWJ79HFpOG9ebQwbkM7Z1NYZ/uDO2TTW43v/fjXFviyWY/ebJJvdpaY8WWHcxatY1Zq0qYtaqEtSW7P7ZOr+xMhvbpzrDe2RT2yY6m+2QzrE82fXt08ash55LMk81+8mTTNu2qrGZNyS5WbdnFmpKdrN66K3qV7GT9tt3EdkidnZXO0N5R4hnaO5uCnK706ZFFfo8u9OmRRd8eXejVPYvMdO/xwLnW4g0EXIeQnZXB2P49Gdu/5yeWVVbXsn77blZv3ZeE1pTs5KPinby2pHjv/aD68rIzowTUPYv8nC7kd8+iT48ue5NSr+wscrpm0LNbJjldM+iRlUGaPy/kXIt4snHtVlZGGsPzuzfYE7WZsaOimi07Ktm6o4ItOyrZsqOCrXXvOyvYUl7J4o1lbCmvoGxPdaPHkaBH1r7kE70y6RneYxNTdlY63TLDe1Y63TLTyc5KJzsrg25Z0bRfWbnOyJON65AkhUSQGdewCJXVtZTsjBLR9l1VlO2ponxPFeV7qinbU03Z7mi6rmxz2R6WF+2br96PAeYy0xWS0L6k1DUzna6ZaXTJSKdLRhpdM6P3LhlpdMlMp2t47xL7nrFv/cz0NLIy0shMF5npafXKoves9Gh5hic7lwKebJwjukrqn9uV/rld93tbM2N3VQ3le6rZVVnDrspq9lTVhOkadlfW7C3fXVnDrqq6suq9y/dU11BRVUvp7ioqqmqpqK5lT1UNFdW1VFTXsKeq4SrBA5EmogSUnkZGSD6ZaeE9JKuMdJGRFjtft86+9dPT0sioK6ubD9P159PrytKjbdIVytJFmqJlaWn71t370r510tPCuulReVrMfuqm09LYW1b3SlPs9ngjkhTxZONcC0kKVymJ+3UyMyproiRUUfXxRFRVY1RW11JVU0tldS2V4b2qJrbMGiirpbrGqK6tparGqK6J3qtqaqmuDe9hu52VNVSH+araWmpqbe+2NbVGdb35qpq22/BIihJSWkxySgvJK018IkEppix2W31sXcL2YTpm/9G6seXUm4/2Fbssdj52ezWyTZoEsfsgeid2H0QxKmZ53X7OO3IwedlZCT3vnmycawckhSqzdNj/i6+UiJLQJ5NRbS0fe6+pNWosWl5r0bo1jb3MqA37q7V95dE0e5d/bP29ZUTTtm/9vcvCe61F+zCr2y+h/OPLamtjpo29MVjMdF3Srdu2bt2Pz+8rq9vWiKbNYuPdt73V26buWAYQs40R7SMe08YUeLJxzrVPUTVWeqrD6PQaSk5mYOwr757Aq/I6HfZOoaQzJC2RtFzStamOxznnUkGhOrCuoUjXzPTQMjKDHl0y6Nk1MylDgXTIZCMpHbgTOBMYD1wgaXxqo3LOuc6rQyYbYDKw3MxWmFkl8ARwdopjcs65Tquj3rMZBKyNmV8HHF1/JUlXAleG2R2Slhzg8fKBLQe4bWfh56hpfn6a5ueneak6R8PiWamjJpuGKiA/0S7DzO4G7m7xwaTZ8fQN1Jn5OWqan5+m+flpXls/Rx21Gm0dMCRmfjCwIUWxOOdcp9dRk80sYLSk4ZKygPOBZ1Mck3POdVodshrNzKolXQ28AKQD95vZwgQessVVcZ2An6Om+flpmp+f5rXpc+Tj2TjnnEu4jlqN5pxzrg3xZOOccy7hPNm0kHeL0zRJqyR9IGmeJB93G5B0v6QiSQtiynpLelHSsvDeK5UxplIj5+cnktaH79E8SWelMsZUkjRE0quSFktaKOnbobxNf4c82bSAd4sTtxPNbEJbfgYgyR4EzqhXdi3wspmNBl4O853Vg3zy/ADcHr5HE8zsuSTH1JZUA98zs3HAFOCq8HenTX+HPNm0jHeL4/abmc0ASuoVnw08FKYfAs5JalBtSCPnxwVmttHM5obpcmAxUa8pbfo75MmmZRrqFmdQimJpqwz4p6Q5oXsg17B+ZrYRoj8mQEGK42mLrpY0P1SztakqolSRVAgcAbxLG/8OebJpmbi6xenkpprZRKKqxqsknZDqgFy7dBcwEpgAbAR+ntpwUk9SD+Ap4BozK0t1PM3xZNMy3i1OM8xsQ3gvAv5MVPXoPmmzpAEA4b0oxfG0KWa22cxqzKwWuIdO/j2SlEmUaB41s6dDcZv+DnmyaRnvFqcJkrpLyqmbBk4DFjS9Vaf1LDA9TE8HnklhLG1O3R/R4HN04u+RJAH3AYvN7LaYRW36O+Q9CLRQaIL5C/Z1i/PTFIfUZkgaQXQ1A1HXSI/5+QFJjwPTiLqE3wxcD/wFeBIYCqwBPm9mnfImeSPnZxpRFZoBq4Cv1t2f6GwkHQf8C/gAqA3F1xHdt2mz3yFPNs455xLOq9Gcc84lnCcb55xzCefJxjnnXMJ5snHOOZdwnmycc84lnCcb1ylJ6hPTg/Cmej0KZ8W5jwckjWlmnaskXdRKMT8gaYyktNbuYVzSlyT1r3+s1jyG69y86bPr9CT9BNhhZj+rVy6i35HaBjdMEUkZwBYzy9vP7dLNrKaRZW8AV5vZvNaI0bn6/MrGuRiSRklaIOm3wFxggKS7Jc0OY4f8V8y6b0iaIClD0nZJt0h6X9LbkgrCOjdLuiZm/VskzQxjIB0byrtLeips+3g41oQGYnsjlN8C5ISrsIfDsulhv/Mk/SZc/dTFdbOkmcBkSTdImlX3GRX5ItEDk3+ou7KLORaSLlY0JtECSf8dyhr9zM41xJONc580HrjPzI4ws/XAtWEsnsOBUxsZsygXeN3MDgfeBr7UyL5lZpOB7wN1ieubwKaw7S1Evfg25VqgPIzrcqmkQ4i6cDnWzCYQ9dZwfkxcc81sspm9DdxhZkcBh4ZlZ5jZH4B5wBfDPiv3BisNBm4GTgxxTZX0mf38zM55snGuAR+Z2ayY+QskzSW60hlHlIzq221mz4fpOUBhI/t+uoF1jiMaCwkzex9YuJ/xngIcBcyWNA/4FFEPyQCV7OsyCODkcJXzfljv4Gb2fTTwipltMbMq4DGgrufueD+zc2SkOgDn2qCddROSRgPfBiab2XZJjwBdG9imMma6hsZ/tyoaWKehoSr2h4j65fvxxwqjezu7LdyYlZQN/BqYaGbrJd1Mw5+l/r4bE+9nds6vbJxrRk+gHCgLPQ+fnoBjvAF8AUDSoTR85bSXmVWHdev+uL8EfEFSfijvI2loA5t2I+q4cUvojfvcmGXlQE4D27wDnBj2WVc993q8H8y5Ov6fiHNNmwssIurSfgXwZgKO8SvgYUnzw/EWAKXNbHMfMF/S7HDf5gbgJUlpQBXwNeqNrWRmWyU9FPa/mqiX4DoPAPdK2k3MWDFmti40iniN6Crnr2b295hE51xcvOmzcykW/nBnmNmeUG33T2B03RWMcx2B/3fiXOr1AF4OSUdEY7V4onEdil/ZOOecSzhvIOCccy7hPNk455xLOE82zjnnEs6TjXPOuYTzZOOccy7h/j+QLj7UJAsXQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24e1badbfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(val_l_g_sweep[:])\n",
    "plt.title('Gradient descent validation loss')\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('Validation loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(tr_l_g_sweep[:])\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('Training loss')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Learn the optimal $P$ and $Q$ using the original stochastic gradient descent (mini-batches of size 1). That is, during each iteration you sample a single random training example $r_{xi}$ and update only the respective affected parameters $\\mathbf{p_x}$ and $\\mathbf{q}_i$. Set the learning rate to 0.01 and keep the other parameters as in a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss: 335340.78, validation loss = 213.70, time = 0.23\n",
      "Iteration 500, training loss: 335306.06, validation loss = 213.68, time = 7.30\n",
      "Iteration 1000, training loss: 335271.99, validation loss = 213.69, time = 6.92\n",
      "Converged after 50 iterations, on average 0.01 per iteration\n"
     ]
    }
   ],
   "source": [
    "Q_g_st, P_g_st, val_l_g_st, tr_l_g_st, conv_g_st = latent_factor_gradient_descent(M_shifted, nonzero_indices, \n",
    "                                                                                   k=30, val_idx=val_idx,\n",
    "                                                                                   val_values=val_values_shifted, \n",
    "                                                                                   reg_lambda=1e-3, learning_rate=1e-1,\n",
    "                                                                                   init='svd', batch_size=1,\n",
    "                                                                                   max_steps=20000, log_every=500, \n",
    "                                                                                   eval_every=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the validation and training losses over (training) time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24e18e0b438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(val_l_g_st[:])\n",
    "plt.title('Gradient descent validation loss')\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('Validation loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(tr_l_g_st[:])\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('Training loss')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) (**Optional**) Learn the optimal $P$ and $Q$ similarly to b) this time using larger mini-batches of size $S$, e.g. 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_g_mb, P_g_mb, val_l_g_mb, tr_l_g_mb, conv_g_mb = latent_factor_gradient_descent(M_shifted, nonzero_indices, \n",
    "                                                                                   k=30, val_idx=val_idx,\n",
    "                                                                                   val_values=val_values_shifted, \n",
    "                                                                                   reg_lambda=1e-3, learning_rate=1e-1,\n",
    "                                                                                   init='svd', batch_size=32,\n",
    "                                                                                   max_steps=10000, log_every=100, \n",
    "                                                                                   eval_every=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the validation and training losses over (training) time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(val_l_g_st[:])\n",
    "plt.title('Gradient descent validation loss')\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('Validation loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(tr_l_g_st[:])\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('Training loss')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models are often heavily dependent on the hyperparameter settings, e.g. the learning rate. Here, we will try a simple random search to find good values of the latent factor dimension $k$, the batch size, learning rate, and regularization.  \n",
    "\n",
    "### Tasks:\n",
    "\n",
    "Perform a hyperparameter search to find good values for the batch size, lambda, learning rate, and latent dimension. \n",
    "\n",
    "* For the batch size, evaluate all values in [1, 32, 512, -1] (-1 corresponds to full-sweep gradient descent).\n",
    "* For $\\lambda$, randomly sample three values in the interval [0, 1).\n",
    "* For the learning rate, evaluate all values in [1, 0.1, 0.01].\n",
    "* For the latent dimension, uniformly sample three values in the interval [5,30].\n",
    "\n",
    "Perform an exhaustive search among all combinations of these values;\n",
    "\n",
    "**Hint**: This may take a while to compute. **You don't have to wait for all the models to train** -- simply use \"dummy\" code instead of actual model training (or let it train, e.g., for only one iteration) if you don't want to wait. Note that the signature of this dummy code has to match the function 'latent_factor_gradient_descent' so that we could simply plug in the actual function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_search(M_train, val_idx, val_values):\n",
    "    \"\"\"\n",
    "    Hyperparameter search using random search.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    M_train     : sp.spmatrix, shape [N, D]\n",
    "                  Input sparse matrix where the user means have not\n",
    "                  been subtracted yet. \n",
    "                  \n",
    "    val_idx     : tuple, shape [2, n_validation]\n",
    "                  The indices used for validation, where n_validation\n",
    "                  is the size of the validation set.\n",
    "                  \n",
    "    val_values  : np.array, shape [n_validation, ]\n",
    "                  Validation set values, where n_validation is the\n",
    "                  size of the validation set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_conf   : tuple, (batch_size, lambda, learning_rate, latent_dimension)\n",
    "                  The best-performing hyperparameters.\n",
    "                  \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE ###  \n",
    "    \n",
    "    param_grid = {\n",
    "          'batch_size': [1, 32, 512, -1],\n",
    "          'regularization_param': [random.uniform(0, 1) for i in range(3)],\n",
    "          'learning_rate': [1, 0.1, 0.01],\n",
    "          'latent_dimension': 5 + np.random.choice(25, 3, replace=False)\n",
    "    }\n",
    "    \n",
    "    grid = ParameterGrid(param_grid)\n",
    "  \n",
    "    best_conf = 0\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for params in grid:\n",
    "        reg = params['regularization_param']\n",
    "        if params['batch_size'] != -1:\n",
    "            reg = reg / 1000\n",
    "        print(\"Training with configuration {:}, {:}, {:}, {:}\".format(params['batch_size'],\n",
    "                                                                      reg,\n",
    "                                                                      params['learning_rate'],\n",
    "                                                                      params['latent_dimension']))    \n",
    "        Q, P, val, tr, conv = latent_factor_gradient_descent(M_train,\n",
    "                                                         nonzero_indices,\n",
    "                                                         k=params['latent_dimension'],\n",
    "                                                         val_idx=val_idx,\n",
    "                                                         val_values=val_values,\n",
    "                                                         reg_lambda= reg,\n",
    "                                                         learning_rate=params['learning_rate'],\n",
    "                                                         batch_size=params['batch_size'],\n",
    "                                                         max_steps=5,\n",
    "                                                         log_every=1,\n",
    "                                                         eval_every=1)\n",
    "        val_0 = min(val)\n",
    "        if(val_0 < best_loss):\n",
    "            best_loss = val_0\n",
    "            best_conf = params\n",
    "    \n",
    "        \n",
    "    print(\"Best configuration is {:}, {:}, {:}, {:}\".format(params['batch_size'],\n",
    "                                                                      reg,\n",
    "                                                                      params['learning_rate'],\n",
    "                                                                      params['latent_dimension']))\n",
    "    return best_conf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with configuration 1, 0.0007561387106549962, 1, 20\n",
      "Iteration 0, training loss: 7697316.67, validation loss = 662.03, time = 0.23\n",
      "Iteration 1, training loss: 8477922.35, validation loss = 662.03, time = 0.29\n",
      "Iteration 2, training loss: 9117425.99, validation loss = 662.03, time = 0.28\n",
      "Iteration 3, training loss: 22324766.64, validation loss = 15753.67, time = 0.29\n",
      "Iteration 4, training loss: 22594214.16, validation loss = 16338.75, time = 0.30\n",
      "Converged after 0 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 5.443630096473862e-05, 1, 20\n",
      "Iteration 0, training loss: 7759827.38, validation loss = 861.49, time = 0.24\n",
      "Iteration 1, training loss: 9810437.75, validation loss = 861.49, time = 0.28\n",
      "Iteration 2, training loss: 10188290.78, validation loss = 861.49, time = 0.29\n",
      "Iteration 3, training loss: 11330162.75, validation loss = 861.49, time = 0.30\n",
      "Iteration 4, training loss: 11907511.67, validation loss = 861.49, time = 0.30\n",
      "Converged after 0 iterations, on average 0.36 per iteration\n",
      "Training with configuration 1, 0.000780688774290563, 1, 20\n",
      "Iteration 0, training loss: 7688457.06, validation loss = 641.69, time = 0.25\n",
      "Iteration 1, training loss: 9415760.37, validation loss = 2768.96, time = 0.29\n",
      "Iteration 2, training loss: 9760333.53, validation loss = 4509.71, time = 0.30\n",
      "Iteration 3, training loss: 10129826.52, validation loss = 4509.71, time = 0.27\n",
      "Iteration 4, training loss: 11394511.25, validation loss = 4509.71, time = 0.28\n",
      "Converged after 0 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 0.0007561387106549962, 0.1, 20\n",
      "Iteration 0, training loss: 7700885.99, validation loss = 799.16, time = 0.25\n",
      "Iteration 1, training loss: 7696944.34, validation loss = 799.16, time = 0.28\n",
      "Iteration 2, training loss: 7685068.43, validation loss = 802.81, time = 0.28\n",
      "Iteration 3, training loss: 7681263.75, validation loss = 802.81, time = 0.28\n",
      "Iteration 4, training loss: 7674388.60, validation loss = 802.81, time = 0.30\n",
      "Converged after 0 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 5.443630096473862e-05, 0.1, 20\n",
      "Iteration 0, training loss: 7686200.75, validation loss = 603.98, time = 0.24\n",
      "Iteration 1, training loss: 7680332.67, validation loss = 600.19, time = 0.28\n",
      "Iteration 2, training loss: 7676136.01, validation loss = 600.19, time = 0.30\n",
      "Iteration 3, training loss: 7663049.75, validation loss = 595.27, time = 0.28\n",
      "Iteration 4, training loss: 7653724.92, validation loss = 595.27, time = 0.27\n",
      "Converged after 3 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 0.000780688774290563, 0.1, 20\n",
      "Iteration 0, training loss: 7688957.85, validation loss = 574.77, time = 0.24\n",
      "Iteration 1, training loss: 7683406.32, validation loss = 574.77, time = 0.28\n",
      "Iteration 2, training loss: 7673784.62, validation loss = 626.60, time = 0.30\n",
      "Iteration 3, training loss: 7669577.10, validation loss = 626.60, time = 0.33\n",
      "Iteration 4, training loss: 7659629.43, validation loss = 626.60, time = 0.28\n",
      "Converged after 0 iterations, on average 0.36 per iteration\n",
      "Training with configuration 1, 0.0007561387106549962, 0.01, 20\n",
      "Iteration 0, training loss: 7668401.09, validation loss = 720.36, time = 0.24\n",
      "Iteration 1, training loss: 7668217.82, validation loss = 720.36, time = 0.28\n",
      "Iteration 2, training loss: 7666701.45, validation loss = 720.36, time = 0.30\n",
      "Iteration 3, training loss: 7666285.29, validation loss = 720.36, time = 0.28\n",
      "Iteration 4, training loss: 7665806.71, validation loss = 720.36, time = 0.28\n",
      "Converged after 0 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 5.443630096473862e-05, 0.01, 20\n",
      "Iteration 0, training loss: 7676608.64, validation loss = 714.55, time = 0.25\n",
      "Iteration 1, training loss: 7674256.69, validation loss = 714.55, time = 0.27\n",
      "Iteration 2, training loss: 7669628.56, validation loss = 714.55, time = 0.30\n",
      "Iteration 3, training loss: 7669068.50, validation loss = 714.55, time = 0.28\n",
      "Iteration 4, training loss: 7668671.55, validation loss = 714.55, time = 0.28\n",
      "Converged after 0 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 0.000780688774290563, 0.01, 20\n",
      "Iteration 0, training loss: 7678794.84, validation loss = 713.09, time = 0.25\n",
      "Iteration 1, training loss: 7678157.61, validation loss = 713.09, time = 0.27\n",
      "Iteration 2, training loss: 7677340.23, validation loss = 713.09, time = 0.28\n",
      "Iteration 3, training loss: 7676951.47, validation loss = 713.09, time = 0.30\n",
      "Iteration 4, training loss: 7676568.76, validation loss = 713.09, time = 0.28\n",
      "Converged after 0 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 0.0007561387106549962, 1, 19\n",
      "Iteration 0, training loss: 7028286.43, validation loss = 530.95, time = 0.24\n",
      "Iteration 1, training loss: 7674644.17, validation loss = 530.95, time = 0.28\n",
      "Iteration 2, training loss: 9339250.53, validation loss = 530.95, time = 0.27\n",
      "Iteration 3, training loss: 9446588.34, validation loss = 530.95, time = 0.28\n",
      "Iteration 4, training loss: 10077209.93, validation loss = 530.95, time = 0.29\n",
      "Converged after 0 iterations, on average 0.34 per iteration\n",
      "Training with configuration 1, 5.443630096473862e-05, 1, 19\n",
      "Iteration 0, training loss: 7027364.07, validation loss = 630.60, time = 0.24\n",
      "Iteration 1, training loss: 8709214.38, validation loss = 630.60, time = 0.27\n",
      "Iteration 2, training loss: 9016962.62, validation loss = 1395.82, time = 0.28\n",
      "Iteration 3, training loss: 9921461.27, validation loss = 1395.82, time = 0.30\n",
      "Iteration 4, training loss: 10021440.15, validation loss = 2203.52, time = 0.28\n",
      "Converged after 0 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 0.000780688774290563, 1, 19\n",
      "Iteration 0, training loss: 7078949.33, validation loss = 556.17, time = 0.24\n",
      "Iteration 1, training loss: 9053691.08, validation loss = 8470.04, time = 0.28\n",
      "Iteration 2, training loss: 9646951.91, validation loss = 9649.89, time = 0.28\n",
      "Iteration 3, training loss: 10142534.19, validation loss = 9649.89, time = 0.28\n",
      "Iteration 4, training loss: 11363773.30, validation loss = 11501.35, time = 0.28\n",
      "Converged after 0 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 0.0007561387106549962, 0.1, 19\n",
      "Iteration 0, training loss: 7061630.78, validation loss = 636.47, time = 0.24\n",
      "Iteration 1, training loss: 7061370.78, validation loss = 636.47, time = 0.28\n",
      "Iteration 2, training loss: 7049164.26, validation loss = 636.47, time = 0.28\n",
      "Iteration 3, training loss: 7038504.36, validation loss = 636.47, time = 0.28\n",
      "Iteration 4, training loss: 7012039.60, validation loss = 636.47, time = 0.30\n",
      "Converged after 0 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 5.443630096473862e-05, 0.1, 19\n",
      "Iteration 0, training loss: 7025120.26, validation loss = 584.81, time = 0.24\n",
      "Iteration 1, training loss: 7010720.35, validation loss = 600.87, time = 0.28\n",
      "Iteration 2, training loss: 7004837.22, validation loss = 600.87, time = 0.28\n",
      "Iteration 3, training loss: 7001258.64, validation loss = 600.87, time = 0.28\n",
      "Iteration 4, training loss: 6997042.60, validation loss = 600.87, time = 0.28\n",
      "Converged after 0 iterations, on average 0.34 per iteration\n",
      "Training with configuration 1, 0.000780688774290563, 0.1, 19\n",
      "Iteration 0, training loss: 7114003.97, validation loss = 644.82, time = 0.24\n",
      "Iteration 1, training loss: 7110723.36, validation loss = 644.82, time = 0.29\n",
      "Iteration 2, training loss: 7109444.20, validation loss = 644.82, time = 0.28\n",
      "Iteration 3, training loss: 7105684.54, validation loss = 644.82, time = 0.29\n",
      "Iteration 4, training loss: 7102407.38, validation loss = 644.82, time = 0.29\n",
      "Converged after 0 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 0.0007561387106549962, 0.01, 19\n",
      "Iteration 0, training loss: 7099243.37, validation loss = 575.98, time = 0.24\n",
      "Iteration 1, training loss: 7096050.79, validation loss = 573.73, time = 0.28\n",
      "Iteration 2, training loss: 7095261.81, validation loss = 573.73, time = 0.30\n",
      "Iteration 3, training loss: 7094939.66, validation loss = 573.73, time = 0.29\n",
      "Iteration 4, training loss: 7094136.47, validation loss = 573.73, time = 0.28\n",
      "Converged after 1 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 5.443630096473862e-05, 0.01, 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss: 7085736.38, validation loss = 724.56, time = 0.24\n",
      "Iteration 1, training loss: 7085571.32, validation loss = 724.56, time = 0.28\n",
      "Iteration 2, training loss: 7084052.82, validation loss = 724.56, time = 0.30\n",
      "Iteration 3, training loss: 7083964.25, validation loss = 724.56, time = 0.28\n",
      "Iteration 4, training loss: 7081314.71, validation loss = 724.56, time = 0.28\n",
      "Converged after 0 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 0.000780688774290563, 0.01, 19\n",
      "Iteration 0, training loss: 7033535.80, validation loss = 592.61, time = 0.23\n",
      "Iteration 1, training loss: 7032927.57, validation loss = 591.63, time = 0.28\n",
      "Iteration 2, training loss: 7032185.94, validation loss = 591.63, time = 0.27\n",
      "Iteration 3, training loss: 7031878.61, validation loss = 591.63, time = 0.28\n",
      "Iteration 4, training loss: 7031012.29, validation loss = 591.63, time = 0.28\n",
      "Converged after 1 iterations, on average 0.34 per iteration\n",
      "Training with configuration 1, 0.0007561387106549962, 1, 27\n",
      "Iteration 0, training loss: 13633442.49, validation loss = 2287.61, time = 0.24\n",
      "Iteration 1, training loss: 20097533.70, validation loss = 2287.61, time = 0.28\n",
      "Iteration 2, training loss: 24190320.82, validation loss = 16094.77, time = 0.28\n",
      "Iteration 3, training loss: 28519888.61, validation loss = 16094.77, time = 0.30\n",
      "Iteration 4, training loss: 30244173.81, validation loss = 16094.77, time = 0.28\n",
      "Converged after 0 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 5.443630096473862e-05, 1, 27\n",
      "Iteration 0, training loss: 13803652.59, validation loss = 2169.40, time = 0.24\n",
      "Iteration 1, training loss: 14496555.90, validation loss = 2169.40, time = 0.28\n",
      "Iteration 2, training loss: 15867421.00, validation loss = 2169.40, time = 0.28\n",
      "Iteration 3, training loss: 28861290.78, validation loss = 2169.40, time = 0.30\n",
      "Iteration 4, training loss: 40915936.73, validation loss = 14483.01, time = 0.30\n",
      "Converged after 0 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 0.000780688774290563, 1, 27\n",
      "Iteration 0, training loss: 13642590.77, validation loss = 2090.35, time = 0.24\n",
      "Iteration 1, training loss: 15163203.68, validation loss = 2090.35, time = 0.28\n",
      "Iteration 2, training loss: 18247063.73, validation loss = 2090.35, time = 0.30\n",
      "Iteration 3, training loss: 23836342.61, validation loss = 2090.35, time = 0.27\n",
      "Iteration 4, training loss: 28000350.89, validation loss = 2090.35, time = 0.30\n",
      "Converged after 0 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 0.0007561387106549962, 0.1, 27\n",
      "Iteration 0, training loss: 13700029.02, validation loss = 2169.33, time = 0.24\n",
      "Iteration 1, training loss: 13705517.77, validation loss = 2169.33, time = 0.28\n",
      "Iteration 2, training loss: 13702279.84, validation loss = 2169.33, time = 0.28\n",
      "Iteration 3, training loss: 13699399.31, validation loss = 2169.33, time = 0.28\n",
      "Iteration 4, training loss: 13690455.05, validation loss = 2169.33, time = 0.28\n",
      "Converged after 0 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 5.443630096473862e-05, 0.1, 27\n",
      "Iteration 0, training loss: 13696132.42, validation loss = 2180.74, time = 0.25\n",
      "Iteration 1, training loss: 13687545.25, validation loss = 2208.17, time = 0.28\n",
      "Iteration 2, training loss: 13685525.57, validation loss = 2208.17, time = 0.31\n",
      "Iteration 3, training loss: 13651351.17, validation loss = 2209.00, time = 0.28\n",
      "Iteration 4, training loss: 13655259.69, validation loss = 2209.00, time = 0.27\n",
      "Converged after 0 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 0.000780688774290563, 0.1, 27\n",
      "Iteration 0, training loss: 13743613.87, validation loss = 2326.29, time = 0.25\n",
      "Iteration 1, training loss: 13742277.06, validation loss = 2326.29, time = 0.28\n",
      "Iteration 2, training loss: 13733554.87, validation loss = 2326.29, time = 0.28\n",
      "Iteration 3, training loss: 13721386.37, validation loss = 2376.09, time = 0.28\n",
      "Iteration 4, training loss: 13704344.58, validation loss = 2376.09, time = 0.28\n",
      "Converged after 0 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 0.0007561387106549962, 0.01, 27\n",
      "Iteration 0, training loss: 13710660.93, validation loss = 2080.30, time = 0.24\n",
      "Iteration 1, training loss: 13700351.32, validation loss = 2080.30, time = 0.27\n",
      "Iteration 2, training loss: 13694425.64, validation loss = 2080.30, time = 0.28\n",
      "Iteration 3, training loss: 13693317.23, validation loss = 2080.30, time = 0.28\n",
      "Iteration 4, training loss: 13690080.33, validation loss = 2080.30, time = 0.28\n",
      "Converged after 0 iterations, on average 0.34 per iteration\n",
      "Training with configuration 1, 5.443630096473862e-05, 0.01, 27\n",
      "Iteration 0, training loss: 13977165.44, validation loss = 2309.77, time = 0.25\n",
      "Iteration 1, training loss: 13971497.06, validation loss = 2309.77, time = 0.28\n",
      "Iteration 2, training loss: 13961319.82, validation loss = 2305.69, time = 0.29\n",
      "Iteration 3, training loss: 13951578.44, validation loss = 2300.06, time = 0.28\n",
      "Iteration 4, training loss: 13948951.85, validation loss = 2300.06, time = 0.28\n",
      "Converged after 3 iterations, on average 0.35 per iteration\n",
      "Training with configuration 1, 0.000780688774290563, 0.01, 27\n",
      "Iteration 0, training loss: 13527959.71, validation loss = 2207.60, time = 0.22\n",
      "Iteration 1, training loss: 13521460.24, validation loss = 2207.60, time = 0.28\n",
      "Iteration 2, training loss: 13520783.16, validation loss = 2207.60, time = 0.29\n",
      "Iteration 3, training loss: 13516977.65, validation loss = 2204.51, time = 0.29\n",
      "Iteration 4, training loss: 13513951.04, validation loss = 2202.08, time = 0.28\n",
      "Converged after 4 iterations, on average 0.34 per iteration\n",
      "Training with configuration 32, 0.0007561387106549962, 1, 20\n",
      "Iteration 0, training loss: 7715515.97, validation loss = 708.11, time = 0.24\n",
      "Iteration 1, training loss: 7625277.33, validation loss = 720.34, time = 0.28\n",
      "Iteration 2, training loss: 7512008.76, validation loss = 706.35, time = 0.30\n",
      "Iteration 3, training loss: 7412999.90, validation loss = 702.81, time = 0.30\n",
      "Iteration 4, training loss: 7302021.52, validation loss = 681.58, time = 0.28\n",
      "Converged after 4 iterations, on average 0.35 per iteration\n",
      "Training with configuration 32, 5.443630096473862e-05, 1, 20\n",
      "Iteration 0, training loss: 7807061.66, validation loss = 740.18, time = 0.23\n",
      "Iteration 1, training loss: 7669954.51, validation loss = 721.60, time = 0.29\n",
      "Iteration 2, training loss: 7556738.23, validation loss = 721.43, time = 0.29\n",
      "Iteration 3, training loss: 7459998.08, validation loss = 709.80, time = 0.28\n",
      "Iteration 4, training loss: 7362985.42, validation loss = 715.54, time = 0.30\n",
      "Converged after 3 iterations, on average 0.35 per iteration\n",
      "Training with configuration 32, 0.000780688774290563, 1, 20\n",
      "Iteration 0, training loss: 7725068.15, validation loss = 727.66, time = 0.25\n",
      "Iteration 1, training loss: 7578835.65, validation loss = 685.60, time = 0.29\n",
      "Iteration 2, training loss: 7456043.21, validation loss = 699.87, time = 0.30\n",
      "Iteration 3, training loss: 7357665.44, validation loss = 682.54, time = 0.36\n",
      "Iteration 4, training loss: 7257684.56, validation loss = 682.03, time = 0.35\n",
      "Converged after 4 iterations, on average 0.39 per iteration\n",
      "Training with configuration 32, 0.0007561387106549962, 0.1, 20\n",
      "Iteration 0, training loss: 7740554.33, validation loss = 721.39, time = 0.25\n",
      "Iteration 1, training loss: 7725720.79, validation loss = 718.83, time = 0.36\n",
      "Iteration 2, training loss: 7710753.13, validation loss = 713.13, time = 0.36\n",
      "Iteration 3, training loss: 7691374.71, validation loss = 710.56, time = 0.33\n",
      "Iteration 4, training loss: 7681348.08, validation loss = 705.22, time = 0.36\n",
      "Converged after 4 iterations, on average 0.42 per iteration\n",
      "Training with configuration 32, 5.443630096473862e-05, 0.1, 20\n",
      "Iteration 0, training loss: 7737154.40, validation loss = 767.17, time = 0.27\n",
      "Iteration 1, training loss: 7723358.08, validation loss = 762.82, time = 0.36\n",
      "Iteration 2, training loss: 7705177.03, validation loss = 758.89, time = 0.29\n",
      "Iteration 3, training loss: 7692932.78, validation loss = 757.27, time = 0.34\n",
      "Iteration 4, training loss: 7679261.99, validation loss = 755.05, time = 0.33\n",
      "Converged after 4 iterations, on average 0.40 per iteration\n",
      "Training with configuration 32, 0.000780688774290563, 0.1, 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss: 7768001.46, validation loss = 593.22, time = 0.29\n",
      "Iteration 1, training loss: 7754066.40, validation loss = 586.55, time = 0.36\n",
      "Iteration 2, training loss: 7734940.85, validation loss = 583.00, time = 0.35\n",
      "Iteration 3, training loss: 7719143.10, validation loss = 582.18, time = 0.36\n",
      "Iteration 4, training loss: 7708523.08, validation loss = 580.26, time = 0.33\n",
      "Converged after 4 iterations, on average 0.42 per iteration\n",
      "Training with configuration 32, 0.0007561387106549962, 0.01, 20\n",
      "Iteration 0, training loss: 7670704.06, validation loss = 786.49, time = 0.29\n",
      "Iteration 1, training loss: 7669575.59, validation loss = 786.46, time = 0.35\n",
      "Iteration 2, training loss: 7667785.48, validation loss = 786.19, time = 0.36\n",
      "Iteration 3, training loss: 7666358.12, validation loss = 785.68, time = 0.35\n",
      "Iteration 4, training loss: 7665192.17, validation loss = 785.53, time = 0.35\n",
      "Converged after 4 iterations, on average 0.43 per iteration\n",
      "Training with configuration 32, 5.443630096473862e-05, 0.01, 20\n",
      "Iteration 0, training loss: 7673430.62, validation loss = 744.44, time = 0.28\n",
      "Iteration 1, training loss: 7672051.32, validation loss = 744.16, time = 0.34\n",
      "Iteration 2, training loss: 7670201.15, validation loss = 743.79, time = 0.37\n",
      "Iteration 3, training loss: 7668928.59, validation loss = 743.37, time = 0.28\n",
      "Iteration 4, training loss: 7667142.88, validation loss = 743.20, time = 0.28\n",
      "Converged after 4 iterations, on average 0.39 per iteration\n",
      "Training with configuration 32, 0.000780688774290563, 0.01, 20\n",
      "Iteration 0, training loss: 7724535.87, validation loss = 811.51, time = 0.28\n",
      "Iteration 1, training loss: 7723391.52, validation loss = 811.19, time = 0.33\n",
      "Iteration 2, training loss: 7721868.22, validation loss = 810.79, time = 0.31\n",
      "Iteration 3, training loss: 7720485.54, validation loss = 810.48, time = 0.29\n",
      "Iteration 4, training loss: 7719045.55, validation loss = 810.26, time = 0.35\n",
      "Converged after 4 iterations, on average 0.40 per iteration\n",
      "Training with configuration 32, 0.0007561387106549962, 1, 19\n",
      "Iteration 0, training loss: 7052744.09, validation loss = 643.69, time = 0.26\n",
      "Iteration 1, training loss: 6922864.42, validation loss = 628.99, time = 0.35\n",
      "Iteration 2, training loss: 6838270.29, validation loss = 620.06, time = 0.30\n",
      "Iteration 3, training loss: 6728165.37, validation loss = 614.86, time = 0.30\n",
      "Iteration 4, training loss: 6642041.74, validation loss = 619.45, time = 0.28\n",
      "Converged after 3 iterations, on average 0.38 per iteration\n",
      "Training with configuration 32, 5.443630096473862e-05, 1, 19\n",
      "Iteration 0, training loss: 7121392.21, validation loss = 705.14, time = 0.23\n",
      "Iteration 1, training loss: 6989405.77, validation loss = 711.14, time = 0.36\n",
      "Iteration 2, training loss: 6891235.71, validation loss = 687.56, time = 0.35\n",
      "Iteration 3, training loss: 6767747.95, validation loss = 687.07, time = 0.36\n",
      "Iteration 4, training loss: 6693079.21, validation loss = 677.18, time = 0.34\n",
      "Converged after 4 iterations, on average 0.41 per iteration\n",
      "Training with configuration 32, 0.000780688774290563, 1, 19\n",
      "Iteration 0, training loss: 7020190.92, validation loss = 606.19, time = 0.24\n",
      "Iteration 1, training loss: 6916307.03, validation loss = 604.86, time = 0.36\n",
      "Iteration 2, training loss: 6817748.45, validation loss = 602.35, time = 0.30\n",
      "Iteration 3, training loss: 6738767.01, validation loss = 607.70, time = 0.34\n",
      "Iteration 4, training loss: 6640352.59, validation loss = 598.93, time = 0.35\n",
      "Converged after 4 iterations, on average 0.40 per iteration\n",
      "Training with configuration 32, 0.0007561387106549962, 0.1, 19\n",
      "Iteration 0, training loss: 7047505.21, validation loss = 558.91, time = 0.28\n",
      "Iteration 1, training loss: 7034450.67, validation loss = 559.05, time = 0.29\n",
      "Iteration 2, training loss: 7023334.14, validation loss = 559.15, time = 0.31\n",
      "Iteration 3, training loss: 7010697.19, validation loss = 557.11, time = 0.30\n",
      "Iteration 4, training loss: 7002112.95, validation loss = 555.83, time = 0.29\n",
      "Converged after 4 iterations, on average 0.38 per iteration\n",
      "Training with configuration 32, 5.443630096473862e-05, 0.1, 19\n",
      "Iteration 0, training loss: 7016427.55, validation loss = 612.71, time = 0.24\n",
      "Iteration 1, training loss: 7001373.36, validation loss = 609.24, time = 0.35\n",
      "Iteration 2, training loss: 6985851.61, validation loss = 607.88, time = 0.30\n",
      "Iteration 3, training loss: 6975962.70, validation loss = 607.94, time = 0.29\n",
      "Iteration 4, training loss: 6967888.95, validation loss = 608.23, time = 0.31\n",
      "Converged after 2 iterations, on average 0.37 per iteration\n",
      "Training with configuration 32, 0.000780688774290563, 0.1, 19\n",
      "Iteration 0, training loss: 7038545.12, validation loss = 613.96, time = 0.24\n",
      "Iteration 1, training loss: 7023457.14, validation loss = 613.37, time = 0.28\n",
      "Iteration 2, training loss: 7010576.38, validation loss = 612.40, time = 0.30\n",
      "Iteration 3, training loss: 6999550.76, validation loss = 610.13, time = 0.31\n",
      "Iteration 4, training loss: 6988092.10, validation loss = 607.70, time = 0.30\n",
      "Converged after 4 iterations, on average 0.36 per iteration\n",
      "Training with configuration 32, 0.0007561387106549962, 0.01, 19\n",
      "Iteration 0, training loss: 7004571.47, validation loss = 607.88, time = 0.24\n",
      "Iteration 1, training loss: 7003336.18, validation loss = 607.63, time = 0.31\n",
      "Iteration 2, training loss: 7001815.38, validation loss = 607.63, time = 0.29\n",
      "Iteration 3, training loss: 7000322.21, validation loss = 607.72, time = 0.28\n",
      "Iteration 4, training loss: 6999350.76, validation loss = 607.58, time = 0.30\n",
      "Converged after 4 iterations, on average 0.36 per iteration\n",
      "Training with configuration 32, 5.443630096473862e-05, 0.01, 19\n",
      "Iteration 0, training loss: 6970686.00, validation loss = 615.96, time = 0.24\n",
      "Iteration 1, training loss: 6969408.65, validation loss = 615.83, time = 0.28\n",
      "Iteration 2, training loss: 6968110.19, validation loss = 615.65, time = 0.32\n",
      "Iteration 3, training loss: 6966999.40, validation loss = 615.55, time = 0.37\n",
      "Iteration 4, training loss: 6965379.96, validation loss = 615.26, time = 0.33\n",
      "Converged after 4 iterations, on average 0.39 per iteration\n",
      "Training with configuration 32, 0.000780688774290563, 0.01, 19\n",
      "Iteration 0, training loss: 7040320.44, validation loss = 581.51, time = 0.24\n",
      "Iteration 1, training loss: 7039134.33, validation loss = 581.36, time = 0.28\n",
      "Iteration 2, training loss: 7037887.42, validation loss = 581.05, time = 0.29\n",
      "Iteration 3, training loss: 7036404.93, validation loss = 580.90, time = 0.30\n",
      "Iteration 4, training loss: 7034951.12, validation loss = 580.75, time = 0.28\n",
      "Converged after 4 iterations, on average 0.35 per iteration\n",
      "Training with configuration 32, 0.0007561387106549962, 1, 27\n",
      "Iteration 0, training loss: 13760724.86, validation loss = 2304.99, time = 0.25\n",
      "Iteration 1, training loss: 13488865.36, validation loss = 2289.49, time = 0.27\n",
      "Iteration 2, training loss: 13196393.80, validation loss = 2224.47, time = 0.30\n",
      "Iteration 3, training loss: 13008329.76, validation loss = 2194.61, time = 0.30\n",
      "Iteration 4, training loss: 12745921.41, validation loss = 2138.50, time = 0.30\n",
      "Converged after 4 iterations, on average 0.36 per iteration\n",
      "Training with configuration 32, 5.443630096473862e-05, 1, 27\n",
      "Iteration 0, training loss: 13650933.30, validation loss = 2156.94, time = 0.24\n",
      "Iteration 1, training loss: 13393431.29, validation loss = 2139.54, time = 0.30\n",
      "Iteration 2, training loss: 13170715.51, validation loss = 2098.22, time = 0.30\n",
      "Iteration 3, training loss: 12908496.66, validation loss = 2069.29, time = 0.30\n",
      "Iteration 4, training loss: 12653581.42, validation loss = 1972.61, time = 0.28\n",
      "Converged after 4 iterations, on average 0.36 per iteration\n",
      "Training with configuration 32, 0.000780688774290563, 1, 27\n",
      "Iteration 0, training loss: 13572394.48, validation loss = 1922.06, time = 0.23\n",
      "Iteration 1, training loss: 13294361.40, validation loss = 1881.12, time = 0.30\n",
      "Iteration 2, training loss: 13012586.31, validation loss = 1816.04, time = 0.29\n",
      "Iteration 3, training loss: 12730777.36, validation loss = 1802.41, time = 0.31\n",
      "Iteration 4, training loss: 12526983.44, validation loss = 1731.44, time = 0.29\n",
      "Converged after 4 iterations, on average 0.36 per iteration\n",
      "Training with configuration 32, 0.0007561387106549962, 0.1, 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss: 13775668.42, validation loss = 2260.09, time = 0.24\n",
      "Iteration 1, training loss: 13734561.73, validation loss = 2248.39, time = 0.28\n",
      "Iteration 2, training loss: 13709276.15, validation loss = 2244.24, time = 0.31\n",
      "Iteration 3, training loss: 13676724.40, validation loss = 2219.52, time = 0.28\n",
      "Iteration 4, training loss: 13644394.00, validation loss = 2204.94, time = 0.30\n",
      "Converged after 4 iterations, on average 0.36 per iteration\n",
      "Training with configuration 32, 5.443630096473862e-05, 0.1, 27\n",
      "Iteration 0, training loss: 13779994.51, validation loss = 2203.61, time = 0.24\n",
      "Iteration 1, training loss: 13743434.99, validation loss = 2199.67, time = 0.28\n",
      "Iteration 2, training loss: 13714476.23, validation loss = 2192.65, time = 0.28\n",
      "Iteration 3, training loss: 13692226.83, validation loss = 2187.99, time = 0.32\n",
      "Iteration 4, training loss: 13659382.32, validation loss = 2176.11, time = 0.30\n",
      "Converged after 4 iterations, on average 0.36 per iteration\n",
      "Training with configuration 32, 0.000780688774290563, 0.1, 27\n",
      "Iteration 0, training loss: 13684769.99, validation loss = 2080.50, time = 0.24\n",
      "Iteration 1, training loss: 13647713.74, validation loss = 2049.20, time = 0.30\n",
      "Iteration 2, training loss: 13618770.49, validation loss = 2032.74, time = 0.31\n",
      "Iteration 3, training loss: 13587449.88, validation loss = 2026.87, time = 0.37\n",
      "Iteration 4, training loss: 13556274.79, validation loss = 2023.46, time = 0.38\n",
      "Converged after 4 iterations, on average 0.41 per iteration\n",
      "Training with configuration 32, 0.0007561387106549962, 0.01, 27\n",
      "Iteration 0, training loss: 13646503.35, validation loss = 2233.55, time = 0.25\n",
      "Iteration 1, training loss: 13642653.99, validation loss = 2233.30, time = 0.35\n",
      "Iteration 2, training loss: 13639601.08, validation loss = 2232.50, time = 0.33\n",
      "Iteration 3, training loss: 13636918.24, validation loss = 2231.31, time = 0.30\n",
      "Iteration 4, training loss: 13633224.82, validation loss = 2230.76, time = 0.28\n",
      "Converged after 4 iterations, on average 0.38 per iteration\n",
      "Training with configuration 32, 5.443630096473862e-05, 0.01, 27\n",
      "Iteration 0, training loss: 13512560.37, validation loss = 2104.66, time = 0.23\n",
      "Iteration 1, training loss: 13509686.91, validation loss = 2103.71, time = 0.29\n",
      "Iteration 2, training loss: 13506478.71, validation loss = 2101.11, time = 0.30\n",
      "Iteration 3, training loss: 13503078.22, validation loss = 2100.06, time = 0.28\n",
      "Iteration 4, training loss: 13499870.94, validation loss = 2099.39, time = 0.28\n",
      "Converged after 4 iterations, on average 0.35 per iteration\n",
      "Training with configuration 32, 0.000780688774290563, 0.01, 27\n",
      "Iteration 0, training loss: 13898570.44, validation loss = 2147.99, time = 0.24\n",
      "Iteration 1, training loss: 13894430.32, validation loss = 2146.68, time = 0.29\n",
      "Iteration 2, training loss: 13890364.77, validation loss = 2145.74, time = 0.28\n",
      "Iteration 3, training loss: 13886562.88, validation loss = 2144.32, time = 0.30\n",
      "Iteration 4, training loss: 13882594.83, validation loss = 2142.68, time = 0.30\n",
      "Converged after 4 iterations, on average 0.36 per iteration\n",
      "Training with configuration 512, 0.0007561387106549962, 1, 20\n",
      "Iteration 0, training loss: 7725997.01, validation loss = 708.11, time = 0.24\n",
      "Iteration 1, training loss: 7568404.72, validation loss = 687.38, time = 0.31\n",
      "Iteration 2, training loss: 7431547.24, validation loss = 667.95, time = 0.33\n",
      "Iteration 3, training loss: 7287755.99, validation loss = 659.72, time = 0.33\n",
      "Iteration 4, training loss: 7151299.51, validation loss = 644.50, time = 0.34\n",
      "Converged after 4 iterations, on average 0.41 per iteration\n",
      "Training with configuration 512, 5.443630096473862e-05, 1, 20\n",
      "Iteration 0, training loss: 7682949.11, validation loss = 697.95, time = 0.23\n",
      "Iteration 1, training loss: 7546793.42, validation loss = 670.70, time = 0.34\n",
      "Iteration 2, training loss: 7415849.11, validation loss = 656.65, time = 0.33\n",
      "Iteration 3, training loss: 7280150.48, validation loss = 646.54, time = 0.41\n",
      "Iteration 4, training loss: 7149387.24, validation loss = 626.49, time = 0.44\n",
      "Converged after 4 iterations, on average 0.45 per iteration\n",
      "Training with configuration 512, 0.000780688774290563, 1, 20\n",
      "Iteration 0, training loss: 7707684.64, validation loss = 690.95, time = 0.25\n",
      "Iteration 1, training loss: 7560006.60, validation loss = 668.46, time = 0.35\n",
      "Iteration 2, training loss: 7424542.15, validation loss = 648.61, time = 0.44\n",
      "Iteration 3, training loss: 7292484.31, validation loss = 633.44, time = 0.43\n",
      "Iteration 4, training loss: 7148126.51, validation loss = 611.38, time = 0.44\n",
      "Converged after 4 iterations, on average 0.51 per iteration\n",
      "Training with configuration 512, 0.0007561387106549962, 0.1, 20\n",
      "Iteration 0, training loss: 7791895.67, validation loss = 705.04, time = 0.31\n",
      "Iteration 1, training loss: 7774982.00, validation loss = 702.16, time = 0.44\n",
      "Iteration 2, training loss: 7759289.61, validation loss = 698.83, time = 0.41\n",
      "Iteration 3, training loss: 7743164.05, validation loss = 695.46, time = 0.36\n",
      "Iteration 4, training loss: 7726523.87, validation loss = 692.71, time = 0.34\n",
      "Converged after 4 iterations, on average 0.48 per iteration\n",
      "Training with configuration 512, 5.443630096473862e-05, 0.1, 20\n",
      "Iteration 0, training loss: 7764322.09, validation loss = 727.92, time = 0.26\n",
      "Iteration 1, training loss: 7749780.15, validation loss = 725.69, time = 0.40\n",
      "Iteration 2, training loss: 7735592.85, validation loss = 723.48, time = 0.35\n",
      "Iteration 3, training loss: 7721976.89, validation loss = 721.21, time = 0.38\n",
      "Iteration 4, training loss: 7707746.46, validation loss = 719.05, time = 0.33\n",
      "Converged after 4 iterations, on average 0.44 per iteration\n",
      "Training with configuration 512, 0.000780688774290563, 0.1, 20\n",
      "Iteration 0, training loss: 7762898.80, validation loss = 759.75, time = 0.23\n",
      "Iteration 1, training loss: 7746574.86, validation loss = 756.46, time = 0.34\n",
      "Iteration 2, training loss: 7730608.12, validation loss = 753.44, time = 0.33\n",
      "Iteration 3, training loss: 7714841.97, validation loss = 749.51, time = 0.32\n",
      "Iteration 4, training loss: 7699157.30, validation loss = 746.54, time = 0.34\n",
      "Converged after 4 iterations, on average 0.40 per iteration\n",
      "Training with configuration 512, 0.0007561387106549962, 0.01, 20\n",
      "Iteration 0, training loss: 7710646.06, validation loss = 659.23, time = 0.24\n",
      "Iteration 1, training loss: 7709098.24, validation loss = 659.07, time = 0.32\n",
      "Iteration 2, training loss: 7707603.15, validation loss = 658.78, time = 0.32\n",
      "Iteration 3, training loss: 7706029.57, validation loss = 658.49, time = 0.34\n",
      "Iteration 4, training loss: 7704516.84, validation loss = 658.28, time = 0.33\n",
      "Converged after 4 iterations, on average 0.40 per iteration\n",
      "Training with configuration 512, 5.443630096473862e-05, 0.01, 20\n",
      "Iteration 0, training loss: 7690636.09, validation loss = 775.67, time = 0.23\n",
      "Iteration 1, training loss: 7689212.08, validation loss = 775.44, time = 0.33\n",
      "Iteration 2, training loss: 7687712.74, validation loss = 775.13, time = 0.32\n",
      "Iteration 3, training loss: 7686190.55, validation loss = 774.82, time = 0.32\n",
      "Iteration 4, training loss: 7684703.63, validation loss = 774.61, time = 0.33\n",
      "Converged after 4 iterations, on average 0.40 per iteration\n",
      "Training with configuration 512, 0.000780688774290563, 0.01, 20\n",
      "Iteration 0, training loss: 7707705.60, validation loss = 655.83, time = 0.23\n",
      "Iteration 1, training loss: 7706019.27, validation loss = 655.47, time = 0.34\n",
      "Iteration 2, training loss: 7704451.39, validation loss = 655.22, time = 0.33\n",
      "Iteration 3, training loss: 7702949.83, validation loss = 654.97, time = 0.33\n",
      "Iteration 4, training loss: 7701373.42, validation loss = 654.70, time = 0.33\n",
      "Converged after 4 iterations, on average 0.40 per iteration\n",
      "Training with configuration 512, 0.0007561387106549962, 1, 19\n",
      "Iteration 0, training loss: 7012882.16, validation loss = 601.44, time = 0.24\n",
      "Iteration 1, training loss: 6881826.23, validation loss = 581.05, time = 0.33\n",
      "Iteration 2, training loss: 6744210.40, validation loss = 563.26, time = 0.33\n",
      "Iteration 3, training loss: 6622550.34, validation loss = 548.14, time = 0.33\n",
      "Iteration 4, training loss: 6512613.01, validation loss = 538.16, time = 0.32\n",
      "Converged after 4 iterations, on average 0.40 per iteration\n",
      "Training with configuration 512, 5.443630096473862e-05, 1, 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss: 7045076.06, validation loss = 548.87, time = 0.23\n",
      "Iteration 1, training loss: 6924499.74, validation loss = 531.66, time = 0.33\n",
      "Iteration 2, training loss: 6803895.16, validation loss = 516.94, time = 0.33\n",
      "Iteration 3, training loss: 6692280.36, validation loss = 505.19, time = 0.33\n",
      "Iteration 4, training loss: 6577035.02, validation loss = 497.98, time = 0.33\n",
      "Converged after 4 iterations, on average 0.40 per iteration\n",
      "Training with configuration 512, 0.000780688774290563, 1, 19\n",
      "Iteration 0, training loss: 7063931.16, validation loss = 644.99, time = 0.23\n",
      "Iteration 1, training loss: 6926689.27, validation loss = 622.25, time = 0.33\n",
      "Iteration 2, training loss: 6810465.27, validation loss = 610.31, time = 0.33\n",
      "Iteration 3, training loss: 6681140.93, validation loss = 590.23, time = 0.32\n",
      "Iteration 4, training loss: 6559859.84, validation loss = 579.91, time = 0.32\n",
      "Converged after 4 iterations, on average 0.40 per iteration\n",
      "Training with configuration 512, 0.0007561387106549962, 0.1, 19\n",
      "Iteration 0, training loss: 7057122.34, validation loss = 585.99, time = 0.23\n",
      "Iteration 1, training loss: 7043300.29, validation loss = 584.03, time = 0.32\n",
      "Iteration 2, training loss: 7029505.61, validation loss = 581.50, time = 0.33\n",
      "Iteration 3, training loss: 7016228.39, validation loss = 579.75, time = 0.33\n",
      "Iteration 4, training loss: 7002792.95, validation loss = 578.03, time = 0.32\n",
      "Converged after 4 iterations, on average 0.40 per iteration\n",
      "Training with configuration 512, 5.443630096473862e-05, 0.1, 19\n",
      "Iteration 0, training loss: 6946434.19, validation loss = 631.75, time = 0.23\n",
      "Iteration 1, training loss: 6934340.89, validation loss = 629.81, time = 0.33\n",
      "Iteration 2, training loss: 6922050.15, validation loss = 628.21, time = 0.31\n",
      "Iteration 3, training loss: 6909398.63, validation loss = 626.52, time = 0.32\n",
      "Iteration 4, training loss: 6897629.49, validation loss = 624.64, time = 0.33\n",
      "Converged after 4 iterations, on average 0.40 per iteration\n",
      "Training with configuration 512, 0.000780688774290563, 0.1, 19\n",
      "Iteration 0, training loss: 7058386.95, validation loss = 609.17, time = 0.23\n",
      "Iteration 1, training loss: 7045002.68, validation loss = 607.38, time = 0.33\n",
      "Iteration 2, training loss: 7031070.79, validation loss = 605.32, time = 0.33\n",
      "Iteration 3, training loss: 7017609.02, validation loss = 602.78, time = 0.33\n",
      "Iteration 4, training loss: 7002901.46, validation loss = 600.89, time = 0.33\n",
      "Converged after 4 iterations, on average 0.40 per iteration\n",
      "Training with configuration 512, 0.0007561387106549962, 0.01, 19\n",
      "Iteration 0, training loss: 7015394.78, validation loss = 565.78, time = 0.23\n",
      "Iteration 1, training loss: 7014022.64, validation loss = 565.55, time = 0.32\n",
      "Iteration 2, training loss: 7012668.85, validation loss = 565.32, time = 0.33\n",
      "Iteration 3, training loss: 7011247.60, validation loss = 565.23, time = 0.33\n",
      "Iteration 4, training loss: 7009900.19, validation loss = 565.06, time = 0.33\n",
      "Converged after 4 iterations, on average 0.40 per iteration\n",
      "Training with configuration 512, 5.443630096473862e-05, 0.01, 19\n",
      "Iteration 0, training loss: 7118892.96, validation loss = 656.15, time = 0.23\n",
      "Iteration 1, training loss: 7117655.70, validation loss = 655.97, time = 0.33\n",
      "Iteration 2, training loss: 7116309.91, validation loss = 655.80, time = 0.32\n",
      "Iteration 3, training loss: 7114989.43, validation loss = 655.52, time = 0.33\n",
      "Iteration 4, training loss: 7113719.19, validation loss = 655.39, time = 0.32\n",
      "Converged after 4 iterations, on average 0.40 per iteration\n",
      "Training with configuration 512, 0.000780688774290563, 0.01, 19\n",
      "Iteration 0, training loss: 7079344.65, validation loss = 550.51, time = 0.23\n",
      "Iteration 1, training loss: 7077940.08, validation loss = 550.24, time = 0.33\n",
      "Iteration 2, training loss: 7076554.28, validation loss = 550.02, time = 0.32\n",
      "Iteration 3, training loss: 7075166.65, validation loss = 549.82, time = 0.33\n",
      "Iteration 4, training loss: 7073752.70, validation loss = 549.61, time = 0.32\n",
      "Converged after 4 iterations, on average 0.40 per iteration\n",
      "Training with configuration 512, 0.0007561387106549962, 1, 27\n",
      "Iteration 0, training loss: 13696815.71, validation loss = 2119.72, time = 0.23\n",
      "Iteration 1, training loss: 13331657.02, validation loss = 2007.07, time = 0.34\n",
      "Iteration 2, training loss: 13012562.72, validation loss = 1926.00, time = 0.33\n",
      "Iteration 3, training loss: 12698202.49, validation loss = 1808.74, time = 0.34\n",
      "Iteration 4, training loss: 12379075.93, validation loss = 1737.64, time = 0.34\n",
      "Converged after 4 iterations, on average 0.41 per iteration\n",
      "Training with configuration 512, 5.443630096473862e-05, 1, 27\n",
      "Iteration 0, training loss: 13663101.80, validation loss = 2287.94, time = 0.24\n",
      "Iteration 1, training loss: 13330390.81, validation loss = 2195.87, time = 0.33\n",
      "Iteration 2, training loss: 12996476.23, validation loss = 2076.46, time = 0.33\n",
      "Iteration 3, training loss: 12683098.04, validation loss = 2006.10, time = 0.37\n",
      "Iteration 4, training loss: 12398220.33, validation loss = 1936.42, time = 0.33\n",
      "Converged after 4 iterations, on average 0.42 per iteration\n",
      "Training with configuration 512, 0.000780688774290563, 1, 27\n",
      "Iteration 0, training loss: 13767234.65, validation loss = 2200.28, time = 0.26\n",
      "Iteration 1, training loss: 13403038.47, validation loss = 2117.12, time = 0.34\n",
      "Iteration 2, training loss: 13062726.16, validation loss = 2015.05, time = 0.38\n",
      "Iteration 3, training loss: 12738545.76, validation loss = 1941.12, time = 0.37\n",
      "Iteration 4, training loss: 12412411.91, validation loss = 1876.58, time = 0.33\n",
      "Converged after 4 iterations, on average 0.44 per iteration\n",
      "Training with configuration 512, 0.0007561387106549962, 0.1, 27\n",
      "Iteration 0, training loss: 13650695.11, validation loss = 2213.55, time = 0.29\n",
      "Iteration 1, training loss: 13613242.99, validation loss = 2200.96, time = 0.42\n",
      "Iteration 2, training loss: 13575719.71, validation loss = 2191.49, time = 0.38\n",
      "Iteration 3, training loss: 13534206.48, validation loss = 2179.22, time = 0.39\n",
      "Iteration 4, training loss: 13496417.85, validation loss = 2169.03, time = 0.35\n",
      "Converged after 4 iterations, on average 0.47 per iteration\n",
      "Training with configuration 512, 5.443630096473862e-05, 0.1, 27\n",
      "Iteration 0, training loss: 13558751.52, validation loss = 2142.29, time = 0.26\n",
      "Iteration 1, training loss: 13525735.13, validation loss = 2132.77, time = 0.43\n",
      "Iteration 2, training loss: 13487731.61, validation loss = 2121.60, time = 0.41\n",
      "Iteration 3, training loss: 13455282.81, validation loss = 2113.08, time = 0.42\n",
      "Iteration 4, training loss: 13418443.37, validation loss = 2100.91, time = 0.35\n",
      "Converged after 4 iterations, on average 0.48 per iteration\n",
      "Training with configuration 512, 0.000780688774290563, 0.1, 27\n",
      "Iteration 0, training loss: 13783026.84, validation loss = 2272.48, time = 0.24\n",
      "Iteration 1, training loss: 13745495.59, validation loss = 2259.17, time = 0.34\n",
      "Iteration 2, training loss: 13707452.40, validation loss = 2247.15, time = 0.34\n",
      "Iteration 3, training loss: 13670717.04, validation loss = 2233.90, time = 0.35\n",
      "Iteration 4, training loss: 13636514.40, validation loss = 2225.12, time = 0.35\n",
      "Converged after 4 iterations, on average 0.42 per iteration\n",
      "Training with configuration 512, 0.0007561387106549962, 0.01, 27\n",
      "Iteration 0, training loss: 13479619.74, validation loss = 2239.73, time = 0.27\n",
      "Iteration 1, training loss: 13475975.11, validation loss = 2238.58, time = 0.45\n",
      "Iteration 2, training loss: 13472453.80, validation loss = 2237.44, time = 0.34\n",
      "Iteration 3, training loss: 13469039.27, validation loss = 2236.36, time = 0.35\n",
      "Iteration 4, training loss: 13465504.30, validation loss = 2235.50, time = 0.33\n",
      "Converged after 4 iterations, on average 0.45 per iteration\n",
      "Training with configuration 512, 5.443630096473862e-05, 0.01, 27\n",
      "Iteration 0, training loss: 13757775.30, validation loss = 2118.48, time = 0.25\n",
      "Iteration 1, training loss: 13754333.27, validation loss = 2117.41, time = 0.34\n",
      "Iteration 2, training loss: 13750567.91, validation loss = 2116.40, time = 0.34\n",
      "Iteration 3, training loss: 13747171.10, validation loss = 2115.33, time = 0.34\n",
      "Iteration 4, training loss: 13743552.71, validation loss = 2114.27, time = 0.33\n",
      "Converged after 4 iterations, on average 0.42 per iteration\n",
      "Training with configuration 512, 0.000780688774290563, 0.01, 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss: 13558702.20, validation loss = 2099.73, time = 0.24\n",
      "Iteration 1, training loss: 13555186.14, validation loss = 2098.61, time = 0.35\n",
      "Iteration 2, training loss: 13551430.03, validation loss = 2097.47, time = 0.36\n",
      "Iteration 3, training loss: 13547806.45, validation loss = 2096.58, time = 0.36\n",
      "Iteration 4, training loss: 13544106.59, validation loss = 2095.51, time = 0.43\n",
      "Converged after 4 iterations, on average 0.45 per iteration\n",
      "Training with configuration -1, 0.7561387106549962, 1, 20\n",
      "Iteration 0, training loss: 7849285.63, validation loss = 716.10, time = 0.29\n",
      "Iteration 1, training loss: 13608858990408.68, validation loss = 3738765071.71, time = 1.16\n",
      "Iteration 2, training loss: 2805559375438799949529415491256320.00, validation loss = 73971541929466087610112802816.00, time = 1.41\n",
      "Iteration 3, training loss: 49063106088223510978614392173298961497885193539542246835082145135774645851263263077940192083968.00, validation loss = 117619029634643484828076040008068052116174602107694313144325772938043995796107515349958656.00, time = 1.26\n",
      "Iteration 4, training loss: 339264138791256890715892479837810573193708670762991502373273309784385805076067649475587168257796120808323540031438711956515806986452215696468218754332446335180793653077430755725107503321993771400699621650887575544542837921784723723980223684124741761004423725382270948157203415040.00, validation loss = 97960009195901344558517939316250986656312865164828018794961055792414680034193181349220176545129862120447131909379128483209974850606251497212931452771154442490750081424929231695994634372723352395135055929743570024511051457004395600226115066296727064853912586332922682802176.00, time = 1.19\n",
      "Converged after 0 iterations, on average 1.56 per iteration\n",
      "Training with configuration -1, 0.054436300964738615, 1, 20\n",
      "Iteration 0, training loss: 7738951.24, validation loss = 785.95, time = 0.27\n",
      "Iteration 1, training loss: 13746574758323.89, validation loss = 3298011956.25, time = 1.13\n",
      "Iteration 2, training loss: 3438834831987698062138450139676672.00, validation loss = 82669225779682669078364815360.00, time = 1.22\n",
      "Iteration 3, training loss: 105259716064838255295954281882751873822583977379220299667723886591009978936678989284151400595456.00, validation loss = 228096214905831600645348210973785472169791225281613603924646943746604024007229391144747008.00, time = 1.22\n",
      "Iteration 4, training loss: 3744537463050616306295730766334024650369379356897060909406047138941151529275349093559604680088667891999694003095737050252375518960288541436514790500429118002822083766717152349440505957653081210142544415681483685505859352002138243493090275242403061989548665533826918929119192285184.00, validation loss = 963721758480422990706751998170074723580205945778055549036948901815300762990727397021958538626322618724964355248045995678568677892952158445620667903950096322948867039993389895711163387480926323086908482669819468143415172407998531365670200001844119507358191399377249597652992.00, time = 1.17\n",
      "Converged after 0 iterations, on average 1.49 per iteration\n",
      "Training with configuration -1, 0.7806887742905629, 1, 20\n",
      "Iteration 0, training loss: 7858548.42, validation loss = 790.70, time = 0.24\n",
      "Iteration 1, training loss: 13651432064469.70, validation loss = 4311681490.93, time = 1.10\n",
      "Iteration 2, training loss: 3162509579525204849478449372332032.00, validation loss = 103074610899614407564957057024.00, time = 1.18\n",
      "Iteration 3, training loss: 82083466844760960518727800299235905050498831675393663312415536516342668159805425015903251070976.00, validation loss = 270264307121819453108286144830635324325753577822637368765412944829122056141349832259272704.00, time = 1.17\n",
      "Iteration 4, training loss: 1800175263470734674328183849571329246792198785275105440373983879774325501332575149086778006933469114302149331849326642722700204823311711985037392483853247969038193734173906828859967170155117635425813463267450528506592296310466749095532160855119941038965683253922392185212860628992.00, validation loss = 1001010534346090439778144505702755382245915411565635342118278135924714575718146910269239224421804954564893781589871446140391466061811076412131792854790580316556812500937203438378706339702643050950832990127250914616006535915334038114783347334374846792472923406812201546153984.00, time = 1.13\n",
      "Converged after 0 iterations, on average 1.43 per iteration\n",
      "Training with configuration -1, 0.7561387106549962, 0.1, 20\n",
      "Iteration 0, training loss: 7710802.64, validation loss = 731.88, time = 0.26\n",
      "Iteration 1, training loss: 1133078078.77, validation loss = 334384.23, time = 1.10\n",
      "Iteration 2, training loss: 147277870681105408.00, validation loss = 2726716787594.76, time = 1.24\n",
      "Iteration 3, training loss: 813116114611752971956490187806139408711680.00, validation loss = 1869142108768296858201600478605737984.00, time = 1.28\n",
      "Iteration 4, training loss: 167158638856078245389573093260668409059810464006444563039858274829652085472915682994085105826746042060777136932782080.00, validation loss = 60998745486475675207610926999357997846938311290307232087982559022408813692139610435865741692219739444952956928.00, time = 1.34\n",
      "Converged after 0 iterations, on average 1.54 per iteration\n",
      "Training with configuration -1, 0.054436300964738615, 0.1, 20\n",
      "Iteration 0, training loss: 7735884.99, validation loss = 680.56, time = 0.24\n",
      "Iteration 1, training loss: 1128051923.62, validation loss = 358882.80, time = 1.35\n",
      "Iteration 2, training loss: 156764240911115968.00, validation loss = 2356439234668.81, time = 1.35\n",
      "Iteration 3, training loss: 1037679804138947897216936789958020147707904.00, validation loss = 1489284577187685152761771322798768128.00, time = 1.14\n",
      "Iteration 4, training loss: 362228258993790443865424553335224970734087710468570842739491875250292422686853034599491542880004712846621907227770880.00, validation loss = 76160198481006433537757079039706412017499218573906470035126009870410065146798517328288437368167487217795596288.00, time = 1.27\n",
      "Converged after 0 iterations, on average 1.57 per iteration\n",
      "Training with configuration -1, 0.7806887742905629, 0.1, 20\n",
      "Iteration 0, training loss: 7751665.96, validation loss = 666.08, time = 0.29\n",
      "Iteration 1, training loss: 1070007263.32, validation loss = 354905.14, time = 1.16\n",
      "Iteration 2, training loss: 117739442435977648.00, validation loss = 2238486057174.26, time = 1.19\n",
      "Iteration 3, training loss: 419916920868494651822104225079831566483456.00, validation loss = 951468656546179717879882198554771456.00, time = 1.29\n",
      "Iteration 4, training loss: 23591484629092351112278390523721428560987809028430159876350127748547219879112701529663221692515003689952088883724288.00, validation loss = 8970140587021344056288333633544592559137846922111343586533771968545733066210818932027711021113135987788087296.00, time = 1.47\n",
      "Converged after 0 iterations, on average 1.63 per iteration\n",
      "Training with configuration -1, 0.7561387106549962, 0.01, 20\n",
      "Iteration 0, training loss: 7650613.54, validation loss = 668.91, time = 0.29\n",
      "Iteration 1, training loss: 6040402.36, validation loss = 5522.53, time = 1.14\n",
      "Iteration 2, training loss: 25347033.70, validation loss = 14717.18, time = 1.39\n",
      "Iteration 3, training loss: 60562462.24, validation loss = 58852.93, time = 1.34\n",
      "Iteration 4, training loss: 21888381.76, validation loss = 28187.52, time = 1.28\n",
      "Converged after 0 iterations, on average 1.61 per iteration\n",
      "Training with configuration -1, 0.054436300964738615, 0.01, 20\n",
      "Iteration 0, training loss: 7740152.13, validation loss = 686.22, time = 0.25\n",
      "Iteration 1, training loss: 6340432.57, validation loss = 5818.94, time = 1.15\n",
      "Iteration 2, training loss: 27407901.26, validation loss = 16751.27, time = 1.26\n",
      "Iteration 3, training loss: 63496398.06, validation loss = 73010.69, time = 1.20\n",
      "Iteration 4, training loss: 38068849.32, validation loss = 16182.65, time = 1.28\n",
      "Converged after 0 iterations, on average 1.54 per iteration\n",
      "Training with configuration -1, 0.7806887742905629, 0.01, 20\n",
      "Iteration 0, training loss: 7738721.32, validation loss = 779.68, time = 0.25\n",
      "Iteration 1, training loss: 5802699.80, validation loss = 5913.68, time = 1.11\n",
      "Iteration 2, training loss: 22570666.34, validation loss = 17148.67, time = 1.21\n",
      "Iteration 3, training loss: 66520453.10, validation loss = 80972.37, time = 1.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, training loss: 78485505.11, validation loss = 94029.93, time = 1.28\n",
      "Converged after 0 iterations, on average 1.51 per iteration\n",
      "Training with configuration -1, 0.7561387106549962, 1, 19\n",
      "Iteration 0, training loss: 7018985.47, validation loss = 508.05, time = 0.23\n",
      "Iteration 1, training loss: 10042588741666.87, validation loss = 2620147443.55, time = 1.10\n",
      "Iteration 2, training loss: 1245075708752017790392254863507456.00, validation loss = 28926750816972820892146991104.00, time = 1.25\n",
      "Iteration 3, training loss: 4674343076496616319134937217885021025366497063187316296266889747990138166838453424400308895744.00, validation loss = 13200773032385883504411631313472504609654991632562955571737436218868660450174656938246144.00, time = 1.24\n",
      "Iteration 4, training loss: 313818909146126682745879425108522988210326103993363614045040260675512572741628856831686717853714945591226691172085353743859380149426438475634014756021025709745124905930251013220918650881548071112327297524909761687455808786131143629400761700562739661172669647117721992180006912.00, validation loss = 189929029801888159066283278960700204557571132338207293052902272670053985005665336994857529578161738509386496223713402656945885022960748298137014450516525032789780215526957108220686957512951160311916891285403841402990027188803076625193180666544496870632182739041871986688.00, time = 1.26\n",
      "Converged after 0 iterations, on average 1.51 per iteration\n",
      "Training with configuration -1, 0.054436300964738615, 1, 19\n",
      "Iteration 0, training loss: 7016404.45, validation loss = 534.91, time = 0.26\n",
      "Iteration 1, training loss: 9845604820117.86, validation loss = 2665076509.77, time = 1.19\n",
      "Iteration 2, training loss: 1155405438330448471573660251979776.00, validation loss = 33516582893583841389669515264.00, time = 1.24\n",
      "Iteration 3, training loss: 3688301449896704540553350433674419537802495565507106287398783428577412139591327990373047861248.00, validation loss = 10852650314131533070330358842019091562474784961187646882603328016949304116594596214996992.00, time = 1.29\n",
      "Iteration 4, training loss: 152237244940336742848758446930396857953302240349799406644614327466146821117198569202423563710810647359167159278948258703449814664054238571799462108725594973545602140701503633156038809872581367656307865017278479841786733186001101536972948477123464104227666658129804840526675968.00, validation loss = 58030714308724550334565347545697697203438525281855647905257042368033686919843885783659786528517376101441822960052475099683288245326474343665886668406733556212101674920714470596327932672934728587305826549442350204199063619027908158224413658631343218030449075414238232576.00, time = 1.26\n",
      "Converged after 0 iterations, on average 1.56 per iteration\n",
      "Training with configuration -1, 0.7806887742905629, 1, 19\n",
      "Iteration 0, training loss: 6951358.03, validation loss = 687.72, time = 0.24\n",
      "Iteration 1, training loss: 8001167307192.34, validation loss = 2955762832.51, time = 1.16\n",
      "Iteration 2, training loss: 562420575090837513574338076344320.00, validation loss = 24679299996424287183160999936.00, time = 1.23\n",
      "Iteration 3, training loss: 415955594890600039899661284954906691356161262481096192373530311327869439908004582154175512576.00, validation loss = 2217330116549175901548921849053347240786263681345711184000446620209909775464444616245248.00, time = 1.18\n",
      "Iteration 4, training loss: 218299594106761945435667159962617841061682381448300355247885643101982858453558093303296561340861768575957714890201319425636738994423872245033335222237974557552739505475592128894515817952300371388396925952023196390269600035293173341119863160323461661178010100949364139622400.00, validation loss = 176723861744025046325584330896600617960795914375435299227650959943448524310674618747683720233773203536720369725199259420224779845482059981301192455087168918011523172392917817995524031890456652650019591923073265819114914427306757581277809743167963231517080389389647872.00, time = 1.28\n",
      "Converged after 0 iterations, on average 1.52 per iteration\n",
      "Training with configuration -1, 0.7561387106549962, 0.1, 19\n",
      "Iteration 0, training loss: 7038184.61, validation loss = 605.97, time = 0.24\n",
      "Iteration 1, training loss: 842235799.74, validation loss = 295383.37, time = 1.17\n",
      "Iteration 2, training loss: 60370153168130048.00, validation loss = 1018664253708.17, time = 1.24\n",
      "Iteration 3, training loss: 60698020172304796518625870298658176499712.00, validation loss = 114638215717766407465121312084590592.00, time = 1.29\n",
      "Iteration 4, training loss: 74762710023832958271994418950593713401210837739343001504351571626450571374733539995705231196695111993274469974016.00, validation loss = 22205086637220906140214878462789874412141639636401184173160327225332576003840965861031344972006848150372352.00, time = 1.22\n",
      "Converged after 0 iterations, on average 1.53 per iteration\n",
      "Training with configuration -1, 0.054436300964738615, 0.1, 19\n",
      "Iteration 0, training loss: 7113734.62, validation loss = 640.70, time = 0.25\n",
      "Iteration 1, training loss: 798378567.67, validation loss = 336149.01, time = 1.12\n",
      "Iteration 2, training loss: 44214896857432128.00, validation loss = 1097792934127.99, time = 1.22\n",
      "Iteration 3, training loss: 21508941996043531323705698335110285230080.00, validation loss = 57499232726847652505061596145385472.00, time = 1.19\n",
      "Iteration 4, training loss: 3053964051017454995286420539519508833391987736007697035900125488591288153862028033807373085888310727738128334848.00, validation loss = 1382215562411991115953980727998982420366413496999852301359879818614062998747592612425840641349195592105984.00, time = 1.19\n",
      "Converged after 0 iterations, on average 1.48 per iteration\n",
      "Training with configuration -1, 0.7806887742905629, 0.1, 19\n",
      "Iteration 0, training loss: 6992313.27, validation loss = 584.17, time = 0.23\n",
      "Iteration 1, training loss: 804428951.91, validation loss = 290747.39, time = 1.14\n",
      "Iteration 2, training loss: 47106703898417952.00, validation loss = 859178354411.96, time = 1.20\n",
      "Iteration 3, training loss: 26123455816513409734254762440491916066816.00, validation loss = 39944750825332128516505491568852992.00, time = 1.19\n",
      "Iteration 4, training loss: 5520267961299955710636921012975173635408850551779732517173028361795411394221650078793027580737289765503018991616.00, validation loss = 1025445772160557919804854394160006616736127080818596959853214729408464944704934554725460259543339910561792.00, time = 1.21\n",
      "Converged after 0 iterations, on average 1.48 per iteration\n",
      "Training with configuration -1, 0.7561387106549962, 0.01, 19\n",
      "Iteration 0, training loss: 6984334.85, validation loss = 676.84, time = 0.24\n",
      "Iteration 1, training loss: 4789850.67, validation loss = 4466.52, time = 1.11\n",
      "Iteration 2, training loss: 15217349.15, validation loss = 7840.44, time = 1.18\n",
      "Iteration 3, training loss: 52145229.93, validation loss = 42878.93, time = 1.21\n",
      "Iteration 4, training loss: 54218256.82, validation loss = 36824.02, time = 1.23\n",
      "Converged after 0 iterations, on average 1.48 per iteration\n",
      "Training with configuration -1, 0.054436300964738615, 0.01, 19\n",
      "Iteration 0, training loss: 7053339.93, validation loss = 572.04, time = 0.24\n",
      "Iteration 1, training loss: 4825927.55, validation loss = 4744.79, time = 1.10\n",
      "Iteration 2, training loss: 15719976.27, validation loss = 9747.68, time = 1.30\n",
      "Iteration 3, training loss: 54627698.41, validation loss = 54427.54, time = 1.26\n",
      "Iteration 4, training loss: 76582046.17, validation loss = 53403.77, time = 1.29\n",
      "Converged after 0 iterations, on average 1.55 per iteration\n",
      "Training with configuration -1, 0.7806887742905629, 0.01, 19\n",
      "Iteration 0, training loss: 6973751.60, validation loss = 522.20, time = 0.24\n",
      "Iteration 1, training loss: 5000620.73, validation loss = 5028.09, time = 1.27\n",
      "Iteration 2, training loss: 17948514.65, validation loss = 13521.64, time = 1.35\n",
      "Iteration 3, training loss: 54143650.38, validation loss = 75022.90, time = 1.43\n",
      "Iteration 4, training loss: 58522576.18, validation loss = 28853.38, time = 1.26\n",
      "Converged after 0 iterations, on average 1.64 per iteration\n",
      "Training with configuration -1, 0.7561387106549962, 1, 27\n",
      "Iteration 0, training loss: 13610070.95, validation loss = 2157.52, time = 0.25\n",
      "Iteration 1, training loss: 72860143352349.77, validation loss = 22261081788.38, time = 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, training loss: 427022369910552684253482546716737536.00, validation loss = 13975619376378603764558281572352.00, time = 1.22\n",
      "Iteration 3, training loss: 176430258311919015438779322968337975530837373308550822836143390654417264260472406055056137625588989952.00, validation loss = 623063564654773137767260139782678081360018931025675702897982432145593819332501869674179422322688.00, time = 1.22\n",
      "Iteration 4, training loss: 15997631053245698388802624104116428763815138994195500726995474444890543960171090081870651490632270096416778888165232216482574864424063529425248720723202712497074375650542243926861245965932255964092742935528605548929934046685515856660656222330248127191442357523762808523629297694758660997864639954944.00, validation loss = 7692018800000808024941085452704871869971990039742716487057259593537428881759530419253958877827009481504836183204613138940739544774062232119753664639734807319104811583537114232340234650625001858265910063709011837399914696631586597251451468416268717240519534175037802423452544211367753662070784.00, time = 1.22\n",
      "Converged after 0 iterations, on average 1.50 per iteration\n",
      "Training with configuration -1, 0.054436300964738615, 1, 27\n",
      "Iteration 0, training loss: 13657910.39, validation loss = 2114.93, time = 0.28\n",
      "Iteration 1, training loss: 81469911170936.45, validation loss = 20729678857.91, time = 1.12\n",
      "Iteration 2, training loss: 665686151976554296699955143216463872.00, validation loss = 19565500431461689753797014847488.00, time = 1.25\n",
      "Iteration 3, training loss: 727029325663080042766009491663462669781834986549280781732180546149016850247058576916227600716543295488.00, validation loss = 2589656461189109930432992673420901827536412117036878611562886288430389233711233375584392172273664.00, time = 1.31\n",
      "Iteration 4, training loss: 1195787331651690305479850929735559859686558207174992674867205480966637110390185722211639915680097767921329108781090213380814011215132066728619153825242593474821424160496241218799765530436047030211920192052369507754936903146974395842441097969649198929602388728102999952809170052873185503878471456129024.00, validation loss = 618970671367705594962182747401697028373307196975322319460962380964042628942279523207084546314763575413466291212059914579590310873284386920082364895378332494819744889488020445417481093380988821528841846047421338576892499343945395708518300657678094021800363053949543659543222435825210492180234240.00, time = 1.29\n",
      "Converged after 0 iterations, on average 1.54 per iteration\n",
      "Training with configuration -1, 0.7806887742905629, 1, 27\n",
      "Iteration 0, training loss: 13731989.69, validation loss = 2324.71, time = 0.24\n",
      "Iteration 1, training loss: 87387900723223.50, validation loss = 24278552695.48, time = 1.13\n",
      "Iteration 2, training loss: 862913905386021190122093857115996160.00, validation loss = 24514132144252500169021322166272.00, time = 1.22\n",
      "Iteration 3, training loss: 1670000443501197518926206267294177764806715573902393101084749095263562851924127509802737680437007613952.00, validation loss = 5243582382266937907191451820652987498138656716331508586901939035695027948471470812287543839555584.00, time = 1.21\n",
      "Iteration 4, training loss: 15114557226836252129581484298495322978531261971081958417887577299006607230375619809929284807434654277022755910474353161060978177802137597129346269786759094769020441938516107209006081927489242378383243520830178035409070148206328215849693931541358383403964273417100981311940214323600446749885533891067904.00, validation loss = 8051724645228946453170850706872519745441103963506663596056120799140641951848805202374302447859033223257320711953389179601661279437309113829389878472185139242804740879572967945377453424734218585899030350744181279533143939647628348917274557286460413875791793930046199731588214405608372033181188096.00, time = 1.24\n",
      "Converged after 0 iterations, on average 1.51 per iteration\n",
      "Training with configuration -1, 0.7561387106549962, 0.1, 27\n",
      "Iteration 0, training loss: 13704638.03, validation loss = 2177.15, time = 0.25\n",
      "Iteration 1, training loss: 5536352320.05, validation loss = 1182256.57, time = 1.12\n",
      "Iteration 2, training loss: 21056643449461760000.00, validation loss = 243926412090340.09, time = 1.21\n",
      "Iteration 3, training loss: 2407326827676720320674376510674565640185643532288.00, validation loss = 2720787094427231298971466208777803541774336.00, time = 1.21\n",
      "Iteration 4, training loss: 4396317929422389133621486914819675358773125295557434915715709986851543877152694703823156026324446946187225681158186789369051673027149824.00, validation loss = 963457821587330300276174320558922105343633841290931401521100982357129947538921215728355111654217331310653805047360059477300609024.00, time = 1.20\n",
      "Converged after 0 iterations, on average 1.48 per iteration\n",
      "Training with configuration -1, 0.054436300964738615, 0.1, 27\n",
      "Iteration 0, training loss: 13691899.05, validation loss = 2365.08, time = 0.25\n",
      "Iteration 1, training loss: 6063378141.66, validation loss = 1350514.06, time = 1.11\n",
      "Iteration 2, training loss: 29505093170857672704.00, validation loss = 316477544149129.88, time = 1.20\n",
      "Iteration 3, training loss: 6994287245495129345147222743858936540239453224960.00, validation loss = 6373545453890935539658730252003343350104064.00, time = 1.21\n",
      "Iteration 4, training loss: 112218777264627708437614403001628032732598290079584073785915190512245595255125400248747167017933854716194020572150533183822863125369061376.00, validation loss = 19569424683189329546810951625440244317171969044622022045405263190251409932584761697471471840191747379355932766089571503037453697024.00, time = 1.26\n",
      "Converged after 0 iterations, on average 1.50 per iteration\n",
      "Training with configuration -1, 0.7806887742905629, 0.1, 27\n",
      "Iteration 0, training loss: 13627141.32, validation loss = 2523.13, time = 0.26\n",
      "Iteration 1, training loss: 5462035292.34, validation loss = 1385201.82, time = 1.11\n",
      "Iteration 2, training loss: 19229620310158344192.00, validation loss = 304222207267379.94, time = 1.23\n",
      "Iteration 3, training loss: 1797167475163889486558838820429302705881310298112.00, validation loss = 2724566691896646283276658762059720893136896.00, time = 1.24\n",
      "Iteration 4, training loss: 1810468347728843899537002598216438384420823619663386700952130899610176955602153769902050446934054933141056282057330447735594364405022720.00, validation loss = 445502329188609439683091772932614065361529509337921009058937782514211069839238962996033245167511679385719274223367957626730053632.00, time = 1.23\n",
      "Converged after 0 iterations, on average 1.51 per iteration\n",
      "Training with configuration -1, 0.7561387106549962, 0.01, 27\n",
      "Iteration 0, training loss: 13822719.72, validation loss = 2138.97, time = 0.24\n",
      "Iteration 1, training loss: 18282609.49, validation loss = 16398.77, time = 1.13\n",
      "Iteration 2, training loss: 141679270.62, validation loss = 104026.04, time = 1.23\n",
      "Iteration 3, training loss: 2888069153.13, validation loss = 2076164.11, time = 1.20\n",
      "Iteration 4, training loss: 377974661202605.50, validation loss = 128383123499.59, time = 1.20\n",
      "Converged after 0 iterations, on average 1.50 per iteration\n",
      "Training with configuration -1, 0.054436300964738615, 0.01, 27\n",
      "Iteration 0, training loss: 13650046.46, validation loss = 2352.92, time = 0.25\n",
      "Iteration 1, training loss: 17866668.50, validation loss = 17254.96, time = 1.13\n",
      "Iteration 2, training loss: 138332134.18, validation loss = 117229.02, time = 1.26\n",
      "Iteration 3, training loss: 2812349007.93, validation loss = 1850484.91, time = 1.22\n",
      "Iteration 4, training loss: 395271720736564.25, validation loss = 95335889550.16, time = 1.20\n",
      "Converged after 0 iterations, on average 1.51 per iteration\n",
      "Training with configuration -1, 0.7806887742905629, 0.01, 27\n",
      "Iteration 0, training loss: 13625329.48, validation loss = 2191.47, time = 0.26\n",
      "Iteration 1, training loss: 17808299.29, validation loss = 15125.09, time = 1.10\n",
      "Iteration 2, training loss: 134822648.63, validation loss = 80305.86, time = 1.21\n",
      "Iteration 3, training loss: 2687670280.29, validation loss = 1617384.10, time = 1.23\n",
      "Iteration 4, training loss: 327587121677028.75, validation loss = 76136130719.04, time = 1.21\n",
      "Converged after 0 iterations, on average 1.49 per iteration\n",
      "Best configuration is -1, 0.7806887742905629, 0.01, 27\n"
     ]
    }
   ],
   "source": [
    "best_configuration = parameter_search(M_train, val_idx, val_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output the best hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best batch size: 512\n",
      " Best lambda: 5.443630096473862e-05\n",
      " Best learning rate:1\n",
      " Best latent dimension: 19\n"
     ]
    }
   ],
   "source": [
    "if best_configuration['batch_size'] != -1:\n",
    "    best_configuration['regularization_param'] /= 1000\n",
    "print(\" Best batch size: {:}\\n Best lambda: {:}\\n \\\n",
    "Best learning rate:{:}\\n Best latent dimension: {:}\".format(best_configuration['batch_size'],\n",
    "                                                        best_configuration['regularization_param'],\n",
    "                                                        best_configuration['learning_rate'],\n",
    "                                                        best_configuration['latent_dimension']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison of gradient descent and alternating optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the latent factor model with both alternating optimization and gradient descent, we now compare their results on the training, validation, and test set.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* Compare the root mean square errors (RMSE) for the training, validation, and test sets different settings of $k$ for both alternating optimization and gradient descent. What do you observe?\n",
    "* Compare the test RMSE for the alternating optimization model and the gradient descent model. Which performs better?\n",
    "* Plot the predicted ratings\n",
    "\n",
    "**Hint**: The output values and plots below are the ones we got when testing this sheet. Yours may be different, but if your validation or test RMSE values are larger than 1.5 or 2, it is likely that you have a bug in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE of best gradient descent model: 0.1415100137221931\n",
      "Validation RMSE of best gradient descent model: 1.0854226467030244\n",
      "Test RMSE of best gradient descent model: 1.0865202425621383\n"
     ]
    }
   ],
   "source": [
    "def calculate_rmse(M, idx, prediction):\n",
    "    y = M[idx]\n",
    "    y_pred = prediction[idx]\n",
    "\n",
    "    sse = np.sum(np.power(y - y_pred, 2))\n",
    "    rmse = np.sqrt(sse / len(idx[0]))\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "predicted_a = Q_a.dot(P_a)\n",
    "print(\"Training RMSE of best gradient descent model:\", calculate_rmse(M_n_shifted, nonzero_indices, predicted_a))\n",
    "print(\"Validation RMSE of best gradient descent model:\",calculate_rmse(M_n_shifted, val_idx, predicted_a))\n",
    "print(\"Test RMSE of best gradient descent model:\",calculate_rmse(M_n_shifted, test_idx, predicted_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss: 335918.07, validation loss = 213.71, time = 0.23\n",
      "Iteration 20, training loss: 335911.30, validation loss = 213.71, time = 1.19\n",
      "Iteration 40, training loss: 335904.28, validation loss = 213.71, time = 1.16\n",
      "Iteration 60, training loss: 335897.66, validation loss = 213.71, time = 1.13\n",
      "Iteration 80, training loss: 335890.46, validation loss = 213.71, time = 1.30\n",
      "Iteration 100, training loss: 335882.70, validation loss = 213.71, time = 1.26\n",
      "Iteration 120, training loss: 335874.66, validation loss = 213.71, time = 1.19\n",
      "Iteration 140, training loss: 335867.43, validation loss = 213.71, time = 1.30\n",
      "Iteration 160, training loss: 335860.47, validation loss = 213.71, time = 1.28\n",
      "Iteration 180, training loss: 335853.21, validation loss = 213.71, time = 1.24\n",
      "Iteration 200, training loss: 335846.40, validation loss = 213.71, time = 1.19\n",
      "Iteration 220, training loss: 335840.00, validation loss = 213.71, time = 1.25\n",
      "Iteration 240, training loss: 335831.47, validation loss = 213.71, time = 1.18\n",
      "Iteration 260, training loss: 335824.24, validation loss = 213.71, time = 1.35\n",
      "Iteration 280, training loss: 335815.83, validation loss = 213.71, time = 1.32\n",
      "Iteration 300, training loss: 335807.86, validation loss = 213.71, time = 1.23\n",
      "Iteration 320, training loss: 335800.53, validation loss = 213.71, time = 1.19\n",
      "Iteration 340, training loss: 335792.41, validation loss = 213.71, time = 1.23\n",
      "Iteration 360, training loss: 335785.64, validation loss = 213.71, time = 1.30\n",
      "Iteration 380, training loss: 335778.25, validation loss = 213.70, time = 1.26\n",
      "Iteration 400, training loss: 335769.40, validation loss = 213.70, time = 1.18\n",
      "Iteration 420, training loss: 335762.46, validation loss = 213.70, time = 1.22\n",
      "Iteration 440, training loss: 335755.56, validation loss = 213.70, time = 1.20\n",
      "Iteration 460, training loss: 335747.55, validation loss = 213.70, time = 1.33\n",
      "Iteration 480, training loss: 335739.94, validation loss = 213.70, time = 1.23\n",
      "Iteration 500, training loss: 335732.54, validation loss = 213.70, time = 1.29\n",
      "Iteration 520, training loss: 335725.66, validation loss = 213.70, time = 1.26\n",
      "Iteration 540, training loss: 335717.60, validation loss = 213.70, time = 1.21\n",
      "Iteration 560, training loss: 335709.62, validation loss = 213.70, time = 1.25\n",
      "Iteration 580, training loss: 335701.58, validation loss = 213.70, time = 1.22\n",
      "Iteration 600, training loss: 335694.38, validation loss = 213.70, time = 1.17\n",
      "Iteration 620, training loss: 335686.41, validation loss = 213.70, time = 1.22\n",
      "Iteration 640, training loss: 335678.92, validation loss = 213.70, time = 1.55\n",
      "Iteration 660, training loss: 335670.55, validation loss = 213.70, time = 1.56\n",
      "Iteration 680, training loss: 335662.18, validation loss = 213.70, time = 1.41\n",
      "Iteration 700, training loss: 335654.02, validation loss = 213.70, time = 1.43\n",
      "Iteration 720, training loss: 335645.04, validation loss = 213.70, time = 1.33\n",
      "Iteration 740, training loss: 335638.04, validation loss = 213.70, time = 1.31\n",
      "Iteration 760, training loss: 335630.59, validation loss = 213.70, time = 1.41\n",
      "Iteration 780, training loss: 335621.41, validation loss = 213.70, time = 1.53\n",
      "Iteration 800, training loss: 335613.22, validation loss = 213.70, time = 1.42\n",
      "Iteration 820, training loss: 335604.64, validation loss = 213.70, time = 1.54\n",
      "Iteration 840, training loss: 335596.20, validation loss = 213.70, time = 1.40\n",
      "Iteration 860, training loss: 335587.06, validation loss = 213.70, time = 1.66\n",
      "Iteration 880, training loss: 335578.52, validation loss = 213.70, time = 2.06\n",
      "Iteration 900, training loss: 335569.50, validation loss = 213.70, time = 1.37\n",
      "Iteration 920, training loss: 335560.27, validation loss = 213.70, time = 1.52\n",
      "Iteration 940, training loss: 335551.73, validation loss = 213.69, time = 1.40\n",
      "Iteration 960, training loss: 335543.19, validation loss = 213.70, time = 1.41\n",
      "Iteration 980, training loss: 335534.07, validation loss = 213.70, time = 1.44\n",
      "Iteration 1000, training loss: 335525.72, validation loss = 213.69, time = 1.46\n",
      "Iteration 1020, training loss: 335516.53, validation loss = 213.69, time = 1.42\n",
      "Iteration 1040, training loss: 335508.03, validation loss = 213.69, time = 1.17\n",
      "Iteration 1060, training loss: 335499.34, validation loss = 213.69, time = 1.44\n",
      "Iteration 1080, training loss: 335489.17, validation loss = 213.69, time = 1.19\n",
      "Iteration 1100, training loss: 335480.39, validation loss = 213.69, time = 1.58\n",
      "Iteration 1120, training loss: 335469.77, validation loss = 213.69, time = 1.68\n",
      "Iteration 1140, training loss: 335461.41, validation loss = 213.69, time = 1.43\n",
      "Iteration 1160, training loss: 335452.19, validation loss = 213.69, time = 1.49\n",
      "Iteration 1180, training loss: 335442.41, validation loss = 213.69, time = 1.19\n",
      "Iteration 1200, training loss: 335433.25, validation loss = 213.69, time = 1.21\n",
      "Iteration 1220, training loss: 335423.21, validation loss = 213.68, time = 1.30\n",
      "Iteration 1240, training loss: 335413.17, validation loss = 213.68, time = 1.18\n",
      "Iteration 1260, training loss: 335403.61, validation loss = 213.68, time = 1.16\n",
      "Iteration 1280, training loss: 335393.48, validation loss = 213.68, time = 1.17\n",
      "Iteration 1300, training loss: 335384.36, validation loss = 213.68, time = 1.24\n",
      "Iteration 1320, training loss: 335374.05, validation loss = 213.68, time = 1.13\n",
      "Iteration 1340, training loss: 335364.92, validation loss = 213.68, time = 1.16\n",
      "Iteration 1360, training loss: 335354.79, validation loss = 213.68, time = 1.22\n",
      "Iteration 1380, training loss: 335345.03, validation loss = 213.68, time = 1.15\n",
      "Iteration 1400, training loss: 335334.05, validation loss = 213.68, time = 1.15\n",
      "Iteration 1420, training loss: 335324.23, validation loss = 213.68, time = 1.12\n",
      "Iteration 1440, training loss: 335314.42, validation loss = 213.68, time = 1.15\n",
      "Iteration 1460, training loss: 335305.35, validation loss = 213.68, time = 1.17\n",
      "Iteration 1480, training loss: 335293.39, validation loss = 213.68, time = 1.14\n",
      "Iteration 1500, training loss: 335282.76, validation loss = 213.68, time = 1.14\n",
      "Iteration 1520, training loss: 335271.28, validation loss = 213.68, time = 1.14\n",
      "Iteration 1540, training loss: 335260.73, validation loss = 213.68, time = 1.15\n",
      "Iteration 1560, training loss: 335249.66, validation loss = 213.68, time = 1.16\n",
      "Iteration 1580, training loss: 335238.65, validation loss = 213.68, time = 1.15\n",
      "Iteration 1600, training loss: 335226.99, validation loss = 213.67, time = 1.15\n",
      "Iteration 1620, training loss: 335216.11, validation loss = 213.67, time = 1.16\n",
      "Iteration 1640, training loss: 335204.70, validation loss = 213.67, time = 1.18\n",
      "Iteration 1660, training loss: 335193.14, validation loss = 213.67, time = 1.17\n",
      "Iteration 1680, training loss: 335180.66, validation loss = 213.67, time = 1.15\n",
      "Iteration 1700, training loss: 335169.16, validation loss = 213.67, time = 1.14\n",
      "Iteration 1720, training loss: 335158.88, validation loss = 213.66, time = 1.16\n",
      "Iteration 1740, training loss: 335147.01, validation loss = 213.66, time = 1.14\n",
      "Iteration 1760, training loss: 335135.08, validation loss = 213.66, time = 1.14\n",
      "Iteration 1780, training loss: 335121.11, validation loss = 213.66, time = 1.16\n",
      "Iteration 1800, training loss: 335110.26, validation loss = 213.66, time = 1.14\n",
      "Iteration 1820, training loss: 335097.80, validation loss = 213.66, time = 1.18\n",
      "Iteration 1840, training loss: 335086.02, validation loss = 213.66, time = 1.12\n",
      "Iteration 1860, training loss: 335074.87, validation loss = 213.66, time = 1.14\n",
      "Iteration 1880, training loss: 335060.01, validation loss = 213.65, time = 1.15\n",
      "Iteration 1900, training loss: 335049.79, validation loss = 213.65, time = 1.17\n",
      "Iteration 1920, training loss: 335036.98, validation loss = 213.65, time = 1.13\n",
      "Iteration 1940, training loss: 335025.41, validation loss = 213.65, time = 1.16\n",
      "Iteration 1960, training loss: 335013.42, validation loss = 213.65, time = 1.27\n",
      "Iteration 1980, training loss: 335000.12, validation loss = 213.65, time = 1.27\n",
      "Iteration 2000, training loss: 334988.58, validation loss = 213.65, time = 1.45\n",
      "Iteration 2020, training loss: 334976.77, validation loss = 213.65, time = 1.40\n",
      "Iteration 2040, training loss: 334964.80, validation loss = 213.65, time = 1.17\n",
      "Iteration 2060, training loss: 334952.13, validation loss = 213.65, time = 1.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2080, training loss: 334938.48, validation loss = 213.65, time = 1.53\n",
      "Iteration 2100, training loss: 334926.25, validation loss = 213.65, time = 1.38\n",
      "Iteration 2120, training loss: 334913.21, validation loss = 213.64, time = 1.28\n",
      "Iteration 2140, training loss: 334900.96, validation loss = 213.65, time = 1.29\n",
      "Iteration 2160, training loss: 334887.31, validation loss = 213.64, time = 1.56\n",
      "Iteration 2180, training loss: 334874.02, validation loss = 213.64, time = 1.21\n",
      "Iteration 2200, training loss: 334860.94, validation loss = 213.64, time = 1.32\n",
      "Iteration 2220, training loss: 334848.53, validation loss = 213.64, time = 1.17\n",
      "Iteration 2240, training loss: 334835.82, validation loss = 213.64, time = 1.23\n",
      "Iteration 2260, training loss: 334821.35, validation loss = 213.64, time = 1.30\n",
      "Iteration 2280, training loss: 334807.70, validation loss = 213.64, time = 1.15\n",
      "Iteration 2300, training loss: 334794.77, validation loss = 213.63, time = 1.30\n",
      "Iteration 2320, training loss: 334780.92, validation loss = 213.63, time = 1.31\n",
      "Iteration 2340, training loss: 334764.77, validation loss = 213.63, time = 1.38\n",
      "Iteration 2360, training loss: 334750.54, validation loss = 213.63, time = 1.33\n",
      "Iteration 2380, training loss: 334734.04, validation loss = 213.63, time = 1.42\n",
      "Iteration 2400, training loss: 334720.05, validation loss = 213.63, time = 1.33\n",
      "Iteration 2420, training loss: 334706.21, validation loss = 213.63, time = 1.14\n",
      "Iteration 2440, training loss: 334692.11, validation loss = 213.63, time = 1.14\n",
      "Iteration 2460, training loss: 334677.83, validation loss = 213.63, time = 1.15\n",
      "Iteration 2480, training loss: 334663.93, validation loss = 213.62, time = 1.17\n",
      "Iteration 2500, training loss: 334650.14, validation loss = 213.62, time = 1.15\n",
      "Iteration 2520, training loss: 334636.41, validation loss = 213.62, time = 1.13\n",
      "Iteration 2540, training loss: 334622.91, validation loss = 213.62, time = 1.14\n",
      "Iteration 2560, training loss: 334608.55, validation loss = 213.62, time = 1.16\n",
      "Iteration 2580, training loss: 334594.04, validation loss = 213.62, time = 1.12\n",
      "Iteration 2600, training loss: 334579.44, validation loss = 213.61, time = 1.15\n",
      "Iteration 2620, training loss: 334563.27, validation loss = 213.61, time = 1.14\n",
      "Iteration 2640, training loss: 334546.95, validation loss = 213.61, time = 1.12\n",
      "Iteration 2660, training loss: 334532.92, validation loss = 213.61, time = 1.13\n",
      "Iteration 2680, training loss: 334519.27, validation loss = 213.60, time = 1.14\n",
      "Iteration 2700, training loss: 334504.84, validation loss = 213.60, time = 1.15\n",
      "Iteration 2720, training loss: 334488.40, validation loss = 213.60, time = 1.14\n",
      "Iteration 2740, training loss: 334472.27, validation loss = 213.60, time = 1.13\n",
      "Iteration 2760, training loss: 334457.41, validation loss = 213.60, time = 1.18\n",
      "Iteration 2780, training loss: 334438.57, validation loss = 213.59, time = 1.42\n",
      "Iteration 2800, training loss: 334422.79, validation loss = 213.59, time = 1.28\n",
      "Iteration 2820, training loss: 334406.92, validation loss = 213.59, time = 1.17\n",
      "Iteration 2840, training loss: 334391.15, validation loss = 213.59, time = 1.36\n",
      "Iteration 2860, training loss: 334374.11, validation loss = 213.59, time = 1.16\n",
      "Iteration 2880, training loss: 334357.84, validation loss = 213.59, time = 1.16\n",
      "Iteration 2900, training loss: 334341.91, validation loss = 213.59, time = 1.41\n",
      "Iteration 2920, training loss: 334326.52, validation loss = 213.59, time = 1.34\n",
      "Iteration 2940, training loss: 334308.01, validation loss = 213.58, time = 1.42\n",
      "Iteration 2960, training loss: 334288.15, validation loss = 213.58, time = 1.15\n",
      "Iteration 2980, training loss: 334270.50, validation loss = 213.58, time = 1.22\n",
      "Iteration 3000, training loss: 334252.96, validation loss = 213.58, time = 1.35\n",
      "Iteration 3020, training loss: 334236.17, validation loss = 213.58, time = 1.26\n",
      "Iteration 3040, training loss: 334218.49, validation loss = 213.58, time = 1.13\n",
      "Iteration 3060, training loss: 334199.89, validation loss = 213.58, time = 1.11\n",
      "Iteration 3080, training loss: 334179.79, validation loss = 213.58, time = 1.15\n",
      "Iteration 3100, training loss: 334161.11, validation loss = 213.58, time = 1.24\n",
      "Iteration 3120, training loss: 334143.63, validation loss = 213.58, time = 1.24\n",
      "Iteration 3140, training loss: 334125.65, validation loss = 213.58, time = 1.45\n",
      "Iteration 3160, training loss: 334108.23, validation loss = 213.58, time = 1.43\n",
      "Iteration 3180, training loss: 334091.49, validation loss = 213.58, time = 1.23\n",
      "Iteration 3200, training loss: 334072.96, validation loss = 213.58, time = 1.33\n",
      "Iteration 3220, training loss: 334055.31, validation loss = 213.57, time = 1.25\n",
      "Iteration 3240, training loss: 334037.27, validation loss = 213.57, time = 1.43\n",
      "Iteration 3260, training loss: 334018.47, validation loss = 213.57, time = 1.31\n",
      "Iteration 3280, training loss: 334000.83, validation loss = 213.57, time = 1.17\n",
      "Iteration 3300, training loss: 333981.08, validation loss = 213.57, time = 1.35\n",
      "Iteration 3320, training loss: 333962.47, validation loss = 213.57, time = 1.17\n",
      "Iteration 3340, training loss: 333944.59, validation loss = 213.56, time = 1.50\n",
      "Iteration 3360, training loss: 333927.38, validation loss = 213.56, time = 1.42\n",
      "Iteration 3380, training loss: 333908.32, validation loss = 213.56, time = 1.19\n",
      "Iteration 3400, training loss: 333888.73, validation loss = 213.56, time = 1.30\n",
      "Iteration 3420, training loss: 333866.66, validation loss = 213.56, time = 1.14\n",
      "Iteration 3440, training loss: 333847.56, validation loss = 213.55, time = 1.14\n",
      "Iteration 3460, training loss: 333828.09, validation loss = 213.55, time = 1.12\n",
      "Iteration 3480, training loss: 333808.48, validation loss = 213.55, time = 1.13\n",
      "Iteration 3500, training loss: 333787.13, validation loss = 213.55, time = 1.15\n",
      "Iteration 3520, training loss: 333769.03, validation loss = 213.55, time = 1.15\n",
      "Iteration 3540, training loss: 333748.10, validation loss = 213.55, time = 1.15\n",
      "Iteration 3560, training loss: 333727.38, validation loss = 213.54, time = 1.15\n"
     ]
    }
   ],
   "source": [
    "Q_s, P_s, val_l_g_sweep, tr_l_g_sweep, conv_g_sweep =  latent_factor_gradient_descent(M_shifted, nonzero_indices, \n",
    "                                                                                                   k=best_configuration['latent_dimension'],\n",
    "                                                                                                   val_idx=val_idx,\n",
    "                                                                                                   val_values=val_values_shifted, \n",
    "                                                                                                   reg_lambda=best_configuration['regularization_param'], \n",
    "                                                                                                   learning_rate=best_configuration['learning_rate'],\n",
    "                                                                                                   init='svd', batch_size=best_configuration['batch_size'],\n",
    "                                                                                                   max_steps=10000, log_every=20, \n",
    "                                                                                                   eval_every=20)\n",
    "                                                       \n",
    "predicted_s = Q_s.dot(P_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training RMSE of best gradient descent model:\", calculate_rmse(M_n_shifted, nonzero_indices, predicted_s))\n",
    "print(\"Validation RMSE of best gradient descent model:\",calculate_rmse(M_n_shifted, val_idx, predicted_s))\n",
    "print(\"Test RMSE of best gradient descent model:\",calculate_rmse(M_n_shifted, test_idx, predicted_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots: Prediction vs. ground truth ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(np.array(L[nonzero_indices].T), np.expand_dims(np.array((predicted_a+user_means)[nonzero_indices]), axis=1))\n",
    "plt.title('Training rating vs. predictions by alternating optimization model')\n",
    "plt.xlabel('Ground truth training rating')\n",
    "plt.ylabel('Predicted rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(np.array(L[nonzero_indices].T), np.expand_dims(np.array((predicted_s+user_means)[nonzero_indices]), axis=1)\n",
    "plt.title('Training rating vs. predictions by gradeint descent model')\n",
    "plt.xlabel('Ground truth training rating')\n",
    "plt.ylabel('Predicted rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(np.array(L[test_idx].T), np.expand_dims(np.array((predicted_s+user_means)[test_idx]), axis=1)\n",
    "plt.title('Test rating vs. predictions by gradeint descent model')\n",
    "plt.xlabel('Ground truth training rating')\n",
    "plt.ylabel('Predicted rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.array(L[val_idx].T), np.expand_dims(np.array((predicted_s+user_means)[val_idx]), axis=1)\n",
    "plt.title('Validation rating vs. predictions by gradeint descent model')\n",
    "plt.xlabel('Ground truth training rating')\n",
    "plt.ylabel('Predicted rating')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
